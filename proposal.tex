\documentclass[12pt]{cmuthesis}
%\usepackage{fullpage,cmu-titlepage2}
\usepackage{times}
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

\usepackage[
%backref controls whether the bibliography has a list of backreferences
%to where the citaiton is used
%backref,
pageanchor=true,
plainpages=false,
pdfpagelabels, % makes the status line page labels the same as latex ones
bookmarks,
bookmarksnumbered,
pdfborder=0 0 0,
pdfpagemode=UseOutlines]{hyperref}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amssymb}
\usepackage[bbsets,Dfprime]{math}
\usepackage[prefixflatinterpret,bracketmodalinterpret,modernsign,substopindex,shortmquant,mquantifiertype,mconnectiveformal,bracketinterpret,fixformat,setfixinterpret,modifopindex,seqarrow,seqoptional,sidenotecalculus,abbrseqcontext,shortterms,nosigmaterms,novarterms]{logic}
\usepackage[pretest,nocommandblocks]{progreg}
\usepackage[bracketinterpret,prefixflatinterpret,bracketmodalinterpret,fixformat,differentialdL]{dL}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{verbatim}

\usepackage{tikz}
\usetikzlibrary{shapes,snakes}
\usepackage{pgfplots}
\usepackage{alltt}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{positioning,shadows}
\usetikzlibrary{automata}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{trees,snakes}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{bussproofs}
\usepackage{proof}
\usepackage{lscape}
\usepackage{accents}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newcommand{\bebecomes}{\mathrel{::=}}
\newcommand{\alternative}{~|~}
\newcommand{\ivr}{\psi}
\newcommand{\meps}[2]{\iota{#1}~{#2}}

\usepackage{prettyref}
\newcommand{\rref}[2][]{\prettyref{#2}}
\newrefformat{ch}{Chapter\,\ref{#1}}
\newrefformat{sec}{Section\,\ref{#1}}
\newrefformat{app}{Appendix\,\ref{#1}}
\newrefformat{def}{Def.\,\ref{#1}}
\newrefformat{thm}{Theorem\,\ref{#1}}
\newrefformat{prop}{Proposition\,\ref{#1}}
\newrefformat{lem}{Lemma\,\ref{#1}}
\newrefformat{cor}{Corollary\,\ref{#1}}
\newrefformat{ex}{Example\,\ref{#1}}
\newrefformat{tab}{Table\,\ref{#1}}
\newrefformat{fig}{Fig.\,\ref{#1}}
\newrefformat{case}{case\,\ref{#1}}

\newcommand{\ws}{\nu}
\newcommand{\wt}{\omega}%
\newcommand{\I}{\iconcat[state=\ws]{\stdI}}%
\newcommand{\It}{\iconcat[state=\wt]{\stdI}}%

\newcommand{\allint}{\mathcal{I}}
\newcommand{\allworld}{\mathcal{W}}
\newcommand{\allstate}{\mathcal{S}}
\newcommand{\allregion}{\mathcal{X}}
\newcommand{\allnoms}{\mathcal{N}}
\newcommand{\logname}{\text{\upshape\textsf{dH{\kern-0.05em}L}}\xspace}
\newcommand{\dRL}{dR\mathcal{L}}
\newcommand{\nom}[1]{{\it #1}}
\newcommand{\rexists}[2]{\lexists[\mathbb{R}]{#1}{#2}}
\newcommand{\sexists}[2]{\lexists[\allworld]{#1}{#2}}
\newcommand{\rforall}[2]{\lforall[\mathbb{R}]{#1}{#2}}
\newcommand{\sforall}[2]{\lforall[\allworld]{#1}{#2}}
\newcommand{\mat}[2]{@_{#1}{#2}}
\newcommand{\mbind}[2]{{\downarrow}#1~#2}
\newcommand{\lequiv}{\leftrightarrow}
\newcommand{\mposs}[1]{\lozenge{#1}}
\newcommand{\mnecc}[1]{\square{#1}}
\newcommand{\lnexp}[1]{{#1}}
\newcommand{\lnom}[1]{\widebar{#1}}
\newcommand{\tclass}{\Theta}
\newcommand{\rtclass}{\theta}
\newcommand{\wtclass}{w}
\newcommand{\rtclassi}[1]{\theta_{#1}}
\newcommand{\wtclassi}[1]{\w_{#1}}
\newcommand{\tclassi}[1]{\Theta_{#1}}
\newcommand{\lsvar}[1]{#1}
\newcommand{\pconst}[1]{#1}
\newcommand{\om}{\omega}
\newcommand{\tom}{\tilde{\omega}}
\newcommand{\CdGL}{\textsf{CdGL}\xspace}
\newcommand{\ProofPlex}{ProofPlex\xspace}
\newcommand{\Isabelle}{Isabelle/HOL\xspace}
\newcommand{\VeriPhy}{VeriPhy\xspace}
\newcommand{\ModelPlex}{ModelPlex\xspace}
\newcommand{\dLi}{\ensuremath{\dL_\iota}}
\newcommand{\tint}[2]{\lenvelope{#1}\renvelope{#2}}
\newcommand{\fint}[1]{\lenvelope{#1}\renvelope}
\newcommand{\pint}[1]{\lenvelope{#1}\renvelope} % (#2,#3) \in 

\draftstamp{\today}{DRAFT}
\begin{document}
\frontmatter
\pagestyle{empty}

\title{  {\it \huge Thesis Proposal}\\
{\bf Foundational End-to-End Hybrid Systems Verification and Synthesis}}
\author{Brandon Bohrer}
\date{\today}
\Year{2019}
\trnumber{}

\committee{
Andr\'e Platzer, Chair \\
Stefan Mitsch? \\
Bryan Parno? \\
Jeremy Avigad? \\
Frank Pfenning? \\
Tobias Nipkow?
}

\support{}
\disclaimer{}

% copyright notice generated automatically from Year and author.
% permission added if \permission{} given.

\keywords{hybrid systems, theorem proving, end-to-end verification, hybrid games, hybrid logic}

\maketitle

%\begin{dedication}
%Hi
%\end{dedication}

\begin{abstract}
Cyber-physical systems (CPSs) combining discrete control and continuous physical dynamics are pervasive in modern society: examples include driver assistance in cars, industrial robotics, airborne collision avoidance systems, the electrical grid, and medical devices.
Many of these systems are safety-critical or even life-critical because they operate in close proximity to humans and in some cases perform life-sustaining functions.
Formal safety verification of these systems is a key tool for attaining the strongest possible guarantees that they meet their safety objectives.
As CPSs cement their role in society, the field of formal verification for CPSs has matured too, particularly verification of hybrid systems models, which provide a common formalism for the discrete and continuous aspects of a CPS.
Of the available approaches to verification, hybrid systems theorem-proving in differential dynamic logic is notable for its strong logical foundations and successful application in a number of case studies using the theorem provers \KeYmaera and \KeYmaeraX.

The use of abstract formalisms, such as hybrid systems, has undoubtedly been a boon to formal verification: hybrid systems provide enough abstraction to make verification tractable while providing enough detail to adequately capture realistic control logic and physics.
Yet the verification of abstract models leaves major gaps to confidence in the safety of the real implementations of CPSs.
These gaps can be seen from two sides: On the theoretical side, a theorem is only trustworthy insofar as it faithfully implements its underlying logic and insofar as that logic is \emph{sound}: that is, we must ensure a tool only says a model is safe when the model is truly safe.
On the practical side, safety results about a model are only useful insofar as safety of the model ensures safety of the implementation, i.e., the implementation must be faithful to the formal model.

My thesis is that both gaps are surmountable, and thus: \emph{Hybrid systems theorem-proving supports an end-to-end verification and synthesis process connecting realistic implementations down to bulletproof theoretical foundations}.
The approach is multifaceted, but at its heart is the proof-driven synthesis.
While a model such as a hybrid system (or in our case, hybrid \emph{games}, the generalization of hybrid systems with adversarial dynamics) is well-suited for verification, the gaps vs. implementation must be closed somehow, preferably automatically.
Synthesis allows the process of closing that gap to be automated: A push-button process produces control software that makes provably-safe decisions and monitor software that detects illegal behavior in the environment.
Verified compilation technology lowers that software to the machine-code level while maintaining formal correctness guarantees.
The theoretical backend enabling synthesis of control and monitor software is a constructive logic for differential games \CdGL, which is proven sound.
In the special case of hybrid systems, proofs are backed by mechanized soundness proof.
\end{abstract}

%\begin{acknowledgments}
%Thanks to DARPA and friends for the dough
%\end{acknowledgments}

\pagestyle{plain}

%\tableofcontents
%\listoffigures
%\listoftables
\mainmatter


\chapter{Introduction}
\label{ch:introduction}
Cyber-physical systems (CPSs) combining discrete control and continuous physical dynamics are pervasive in modern society: examples include driver assistance in cars, industrial robotics, airborne collision avoidance systems, the electrical grid, and medical devices.
Many of these systems are safety-critical or even life-critical because they operate in close proximity to humans and in some cases perform life-sustaining functions.
Formal safety verification of these systems is a key tool for attaining the strongest possible guarantees that they meet their safety objectives.
As CPSs cement their role in society, the field of formal verification for CPSs has matured too, particularly verification of hybrid systems models, which provide a common formalism for the discrete and continuous aspects of a CPS.
Of the available approaches to verification, hybrid systems theorem-proving in differential dynamic logic (\dL)~\cite{Platzer18,DBLP:journals/jar/Platzer08,DBLP:journals/jar/Platzer17,DBLP:conf/lics/Platzer12b:TR} is notable for its strong logical foundations and successful application in a number of case studies~\cite{DBLP:conf/emsoft/JeanninGKGSZP15,DBLP:conf/fm/LoosPN11,DBLP:conf/rss/MitschGP13,DBLP:conf/hybrid/PlatzerQ08} using the theorem provers \KeYmaera~\cite{DBLP:conf/cade/PlatzerQ08} and \KeYmaeraX~\cite{DBLP:conf/cade/FultonMQVP15}.

The use of abstract formalisms, such as hybrid systems~\cite{DBLP:conf/lics/Henzinger96}, has undoubtedly been a boon to formal verification: hybrid systems provide enough abstraction to make verification tractable while providing enough detail to adequately capture realistic control logic and physics.
Yet the verification of abstract models leaves major gaps to confidence in the safety of the real implementations of CPSs.
These gaps can be seen from two sides: On the theoretical side, a theorem is only trustworthy insofar as it faithfully implements its underlying logic and insofar as that logic is \emph{sound}: that is, we must ensure a tool only says a model is safe when the model is truly safe.
On the practical side, safety results about a model are only useful insofar as safety of the model ensures safety of the implementation, i.e., the implementation must be faithful to the formal model.

In prior work with my collaborators, I have addressed both aspects of this problem in the context of \emph{classical logics} for hybrid \emph{systems} (i.e., without adversarial dynamics).
On the one hand, our experiences with these prior works have exposed the limitations of those works:
In a classical logic, the connection between proof and computation is loose at best, and does not hold in every case.
As a result, state-of-the-art synthesis technology~\cite{DBLP:journals/fmsd/MitschP16} exploits proof knowledge for synthesis only in limited cases, and even then is fragile.
Developing a computational basis for a hybrid systems logic will give us an exhaustive and robust foundation for synthesis.
Furthermore, the prior emphasis on hybrid \emph{systems} has limited synthesis approaches to monitoring only.
While monitoring is in some sense the most essential synthesis problem, it is greatly limiting especially when we desire robust end-to-end guarantees: an approach that relies only on monitor synthesis can guarantee only safety of the implementation while an approach using games can ensure both safety and \emph{liveness}.
For example, a car which stays in its garage is safe because it never collides with another car but is not live because it never reaches its objective, which might be to drive to a certain destination.
A successful, practical system must not be simply safe because safety guarantees in isolation can be ensured trivially.
The real challenge is to successfully meet the competing challenges of safety and liveness at the same time.

On the foundations side, prior work has explored the classical hybrid systems case in detail:
A formalization~\cite{DBLP:conf/cpp/BohrerRVVP17} of the soundness theorem for \dL has been performed in \Isabelle~\cite{DBLP:books/sp/NipkowPW02}.
Follow-up work~\cite{DBLP:conf/pldi/BohrerTMMP18} enabled the export of proof terms from \KeYmaeraX and rechecking in a verified proof term checker based on the \Isabelle formalization.
That checker supported a significant fragment of the \KeYmaeraX core, including enough to recheck proofs of safety for monitor-based synthesized controllers.
However, this was achieved largely through a lot of sweat and even then failed to cover some popular extensions used in practice.
A recent submission~\cite{hilbert-epsilons} has shown how a redesign of the \dL calculus can enable a simplified and more exhaustive foundation.

Generalizing these results to the case of constructive differential games is an interesting challenge for a number of reasons.
First off, all previous logical explorations of hybrid games~\cite{DBLP:conf/cade/QueselP12,DBLP:journals/tocl/Platzer15,DBLP:journals/tocl/Platzer17,DBLP:conf/cade/Platzer18} have explored only the classical case.
Constructive differential games are the more target because a solution of a constructive game corresponds exactly to a control function and monitor function, while the solution of a classical game corresponds only loosely.
It is arguably a historical accident that hybrid games were discovered in the classical context first due to the large body of work on classical analysis, from which a classical logic for hybrid games followed.
Consequently, one of the major contributions of this thesis is to develop a constructive semantics of hybrid games, of which the main challenge is expected to be a constructive semantics of differential equations using arbitrary-precision verifiable integration bounds.
The subsequent development of a proof calculus for constructive hybrid games is also a contribution of the thesis.

One potential stumbling block in the proposed work is supporting constructive hybrid games in the verified pipeline for code generation, including the verified \dL proof term checker.
In particular it should be noted that the semantics of hybrid games generally and certainly of constructive hybrid games differ profoundly from classical hybrid systems, so supporting games directly would require near-total reimplementation of tools for minimal gain.
To keep the generality and clarity provided by games without excessive implementation effort, I propose a looser integration between the components of the thesis: In much the same way as the prior work \ModelPlex functions, we can use proofs in \CdGL to extract a correct-by-construction monitor formula and control law from a proven-correct model.
When we wish to employ the synthesized code in practice, we can (perhaps automatically) \emph{relax} the constructive game to a classical system, and use existing technology~\cite{DBLP:conf/pldi/BohrerTMMP18} to foundationally check the proof and to generate verified monitors.
Since the controller synthesized from the game will be correct-by-construction, it will satisfy the monitors of the game relaxation except in cases where some compilation bug or conservativity introduced an error.
This then also allows us more freedom in how the low-level details of and compilation of the synthesized controller are handled.

\chapter{Background: Differential Game Logic \dGL}
\emph{Differential dynamic logic} (\dL)~\cite{Platzer18,DBLP:journals/jar/Platzer08,DBLP:journals/jar/Platzer17,DBLP:conf/lics/Platzer12b:TR} is a dynamic logic for the formal verification of hybrid systems, which combine discrete transitions with differential equations to model CPSs.
\emph{Differential game logic} (\dGL)~\cite{DBLP:journals/tocl/Platzer15,DBLP:journals/tocl/Platzer17,DBLP:conf/cade/Platzer18} extends the programs $\alpha$ of \dL with a turn-taking operator $\pdual{\alpha}$ to enable representing and verifying hybrid \emph{games} combining discrete, continuous, and adversarial dynamics.
While some of the works in thesis (\rref{ch:logical-foundations}, \rref{sec:veriphy}) are specific to \dL, we will present \dGL in its full generality here since 
\begin{inparaenum}[i)]
  \item the proposed works (\rref{ch:cdgl},\rref{ch:proofplex}) will require an understanding of full \dGL, and
  \item an introductory understanding of \dGL will also provide an introductory understanding of \dL.
\end{inparaenum}
While the syntax of \dGL and \dL are nearly identical, their semantic differences are deep: \dL is a regular modal logic whose denotational semantics can be and are given in the forward-chaining, relational Kripke style, while \dGL is a subregular logic with a backward-chaining winning-region semantics.
While many basic axioms are the same in \dL and \dGL, some are necessarily different because \dGL is subregular: for example, Kripke's axiom K fails in \dGL and is in practice subsumed by a monoticity rule.
\section{Syntax}
\label{sec:dgl-syntax}
We introduce the syntax and informal meaning of \dGL here, then introduce the winning-region semantics formally in \rref{sec:dgl-semantics}.
The syntax of \dGL consists of three syntactic classes: terms, formulas, and hybrid games.

\begin{definition}[Terms of \dGL]
Terms $\theta, \eta$ of \dGL are defined recursively according to the following grammar:
\[\theta,\eta \bebecomes q \alternative x \alternative \theta + \eta \alternative \theta \cdot \eta \alternative \der{\theta} \]
\end{definition}
Where $q \in \mathbb{Q}$ is a rational constant, $x \in \allvars$ is a program variable and $\allvars$ is the (at most countable) set of all base variable names.
For every base variable $x \in \allvars,$ there is a differential variable $\D{x} \in \D{\allvars}$ standing for the differential of $x$.
Terms $\theta + \eta$ and $\theta \cdot \eta$ are the sum and product of $\theta$ and $\eta,$ and $\der{\theta}$ is the \emph{differential} of term $\theta$.
It is worth noting that the \dGL term language is restrictive, and for good reason: all terms are polynomials, so all terms are defined in every state and even $C^\infty$-smooth.
Because \dGL terms are well-behaved, the theory of \dGL is simplified as a result.
In \rref{sec:definite-description}, we show how to remove these simplifying assumptions in the context of \dL, enabling many more term constructs that are essential in practical proving, the most common constructs including quotients, roots, trigonometric functions, and conditionals.


\begin{definition}[Formulas of \dGL]
  Formulas are defined by the following grammar:
\[\phi,\psi \bebecomes \theta \sim \eta \alternative \phi \land \psi \alternative \phi \lor \psi \alternative \neg \phi \alternative \phi \limply \psi \alternative \phi \lequiv \psi \alternative \lforall{x}{\phi} \alternative \lexists{x}{\phi} \alternative \dbox{\alpha}{\phi} \alternative \ddiamond{\alpha}{\phi}\]
\end{definition}
where $\sim$ stands for any comparison operator $\sim \in \{\leq, <, =, \neq, >, \geq\}$.
We write conjunctions $\phi \land \psi,$ negations $\phi \lor \psi,$ disjunctions $\phi \lor \psi,$ implications $\phi \limply \psi,$ biimplications $\phi \lequiv \psi$, and quantifiers over real numbers $\lexists{x}{\phi}$ and $\lforall{x}{\phi}$.
The diamond modality $\ddiamond{\alpha}{\phi}$ says that the player who is actively making decisions (typically named Angel) has a strategy for the game $\alpha$ which ensures postcondition $\phi$.
The box modality $\dbox{\alpha}{\phi}$ says that the player who is not currently making decisions (typically named Demon) has a strategy for the game $\alpha$ which ensures postcondition $\phi$.
Because \dGL is a classical logic, it is worth noting that the formula syntax is not minimal: many constructs such as implication, equivalence, disjunction, universal quantifiers, and the box modality are all definable classically.
%TODO: cite
This will not be the case in the proposed logic \CdGL because these dualities do not hold constructively.

\begin{definition}[Hybrid games]
Games are defined by the following grammar:  
\[\alpha,\beta \bebecomes \pevolvein{\D{x}=\theta}{\ivr} \alternative \humod{x}{\theta} \alternative \prandom{\alpha} \alternative \ptest{\phi} \alternative \pchoice{\alpha}{\beta} \alternative \alpha;~\beta \alternative \prepeat{\alpha} \alternative \pdual{\alpha}\]
\end{definition} 
where $\humod{x}{\theta}$ stores the current value of term $\theta$ in program variable $x$  and test $\ptest{\phi}$ makes Angel lose if formula $\phi$ does not hold in the current state.
In nondeterministic assignment $\prandom{x},$ Angel chooses a value $r \in \mathbb{R}$ to assign to $x$.
Program $\pevolvein{\D{x}=\theta}{\phi}$ evolves $x$ continuously according to the differential equation (ODE) $\D{x}=\theta$ for a duration $d \geq 0$ of Angel's choosing, but Angel must ensure that $\phi$ is true throughout the evolution of $\D{x}=\theta,$ else they lose.
In game $\pchoice{\alpha}{\beta},$ Angel chooses whether to play game $\alpha$ or game $\beta$.
In game $\alpha;~\beta$ the players must first play $\alpha$ and then play $\beta$; this case is largely responsible for the need to employ backward-chaining semantics in \dGL, as it is most natural to first ask from what region $X$ $\beta$ is winnable, then ask from what region does Angel have a strategy to reach $X$ when playing $\alpha$.
In the iterated game $\prepeat{\alpha}$, Angel decides at the end of each loop iteration whether to continue playing $\alpha$ another time.
All plays must be finite (Angel must stop eventually), but Angel need not decide a-priori \emph{when} to stop, let alone announce that decision to Demon beforehand.
If Angel cannot reach the goal region without repeating $\alpha$ infinitely, then Angel loses.
To play the dual game $\pdual{\alpha},$ Angel and Demon switch roles then play game $\alpha$ in their new roles.
That is, the player previously known as Demon makes the decisions in $\alpha,$ at least until another dual operator is encountered.
We parenthesize hybrid games $\{ \alpha \}$ for clarity and disambiguation as needed.


We now give an example hybrid game and example safety and liveness properties.
The game in \rref{ex:driving-game} is a generalization of a standard hybrid systems model to a hybrid game.
\newcommand{\obsvar}{{\it obs}\xspace}
\newcommand{\Tvar}{{\it T}\xspace}
\newcommand{\Avar}{{\it A}\xspace}
\newcommand{\Bvar}{{\it B}\xspace}
%\newcommand{\SBvar}{{\it SB }\xspace}
\newcommand{\xvar}{{\it x}\xspace}
\newcommand{\vvar}{{\it v}\xspace}
\newcommand{\avar}{{\it a}\xspace}
\begin{example}[Acceleration-Controlled 1D Driving]
\label{ex:driving-game}
\begin{align}
\alpha_\times\equiv&\\
\label{eq:driving-init}\humod{v}{0};~&\humod{x}{0};\pdual{(\prandom{\obsvar};\prandom{T};\prandom{A};\prandom{B}; \ptest{\obsvar > 0 \land T > 0 \land A > 0 \land B > 0})};\\
\label{eq:driving-angel}\big\{ &\prandom{a};~\ptest{-B \leq a \land a \leq A};\\ 
\label{eq:driving-demon}       &\pdual{\humod{t}{0};~\{\pevolvein{\D{x}=v, \D{v}=a, \D{t}=1}{t \leq T \land v \geq 0}\}}\big\}^\times
\end{align}
\end{example}
The first line (\rref{eq:driving-init}) is an initialization phase: we (Angel) call the current position $x = 0$ and are initially stopped ($v = 0$).
The opponent then picks how far away our obstacle (\obsvar) is and how long Angel will have to wait between control decisions ($T$).
They also pick Angel's maximum braking rate $B$ and maximum acceleration rate $B$.
One could argue on principle that neither player really picks the timestep $T,$ in practice this is determined by the speed of the robot's processors, sensors, and actuators, as well as the speed of its control program.
However, what's important is that robot's controller (the Angel player) is \emph{not} the one who picks, so a conservative model should assume the worst, which is that their opponent (the environment) gets to pick.
The same goes for the rates $A$ and $B$.
It would also be eminently unreasonable if the obstacle already collided with the robot at the start of the game, and would be nonsensical if the control cycle lasted zero time or less, or if the robot could not accelerate or brake, thus the environment is responsible for picking positive $\obsvar, \Tvar, \Avar$, and  $\Bvar,$ else they lose by default and Angel wins immediately.
The remainder of the game is a \emph{demonic loop} $\alpha^\times$ where the Demon player controls the duration, which can be derived $\alpha^\times \equiv \pdual{{\prepeat{{\pdual{{\alpha}}}}}}$.
%TODO: write all the derived operators
Angel's turn consists of making a control decision by setting the acceleration $a$.
Angel has near complete freedom to set the acceleration: they may set any value ($\prandom{a}$) so long as it is within the physical limits of the car ($\ptest{-B \leq a \land a \leq A}$).
It is Angel's responsibility to do so, and they lose the game if the acceleration is out of bounds.
Next, the car moves according to ideal Newtonian physics: the velocity $v$ continuously changes according to acceleration $a$ while position changes continuously according to the velocity.
Demon (the environment) controls the duration of the ODE, but is constrained to the timestep $t \leq T$: i.e., Angel's control action must be safe even if Demon chooses not to use the full time budget, but need not be safe past time $T:$ in that case Demon is responsible for breaking the rules, so Angel wins.
The equation $\D{t}=1$ simply says that $t$ represents the time elapsed in the current ODE run.

We write $\alpha_*$ for $\alpha_\times$ with an Angelic loop.

We give examples of safety and liveness theorems:
%TODO: Check these for correctness
\begin{example}[1D Game Safety and Liveness]
  \begin{align*}
    \textit{safe} &\equiv \ddiamond{\alpha_\times}{\xvar \leq \obsvar}\\
    \textit{live} &\equiv \ddiamond{\alpha_*}{(\xvar \geq \obsvar \lor t \leq \frac{T}{2})}
  \end{align*}
\end{example}
The safety theorem \emph{safe} says that Angel (the robot) has a strategy to ensure $\xvar \leq \obsvar$ (i.e. a strategy not to hit the obstacle) regardless how long Demon runs the loop and the ODE.
The liveness theorem \emph{live} says that Angel (the robot) has a strategy to reach the obstacle, \emph{so long as} Demon always runs the ODE long enough ($t \geq \frac{T}{2}$).
The requirement on running the ODE long enough is to rule out \emph{Zeno} behaviors by Demon: without this restriction, Demon can run the ODE for shorter and shorter durations, with infinitely many iterations in finite time, which is not physically realizable but would falsify the liveness theorem.
 

It is worth noting that in the present state of the art, case studies have thus~\cite{DBLP:conf/emsoft/JeanninGKGSZP15,DBLP:conf/fm/LoosPN11,DBLP:conf/rss/MitschGP13,DBLP:conf/hybrid/PlatzerQ08} far used \dL for hybrid systems in \KeYmaeraX (\rref{sec:veriphy},\rref{sec:ground-robotics}).
One of the conclusions of this thesis is that such case studies can and should be written as hybrid games: using games as our modeling language both avoids common anomalies in models and also enables synthesis of not only monitors but also controllers.
The reason for this is that the diamond modality $\ddiamond{\alpha}{\phi}$ can be understood as the \emph{control} modality while box modality $\dbox{\alpha}{\phi}$ can be understood as the \emph{monitoring} modality: that is, while we are the Angel player, it is our responsibilty to make control decisions that will achieve our goal condition, but while we are the Demon player we do not get to make the decisions, it is simply our responsibility to act as a referee for the Angel player, calling them out if they ever break the rules and hand us a victory.

It is illustrative to compare \rref{ex:driving-game} to a previous hybrid system model of the same scenario:
\begin{example}[Acceleration-Controlled 1D Driving System]
\label{ex:driving-system}
\begin{align}
\label{eq:driving-sys-pre}
\textrm{Pre}      & \equiv v=0 \land x=0 \land \obsvar > 0 \land T > 0 \land A > 0 \land B > 0\\
\alpha_{\textrm{Sys}}& \equiv\\
\label{eq:driving-sys-forward}\big\{\big\{~&\prandom{a};~\ptest{-B \leq a \land a \leq A};~\ptest{\frac{(v+a T)^2}{2\Bvar} \leq \obsvar - \left(\xvar + \vvar \Tvar + \avar\frac{\Tvar^2}{2}\right)}\\
\label{eq:driving-sys-stay}     \cup&~\ptest{v = 0};~\humod{a}{0}\\
\label{eq:driving-sys-brake}    \cup&~\ptest{v \geq 0};~\humod{a}{-B}\big\};\\
\label{eq:driving-sys-demon}       &\{\pevolvein{\D{x}=v, \D{v}=a, \D{t}=1}{t \leq T \land v \geq 0}\}\big\}^*
\end{align}
\end{example}
The most salient difference is that the hybrid system $\alpha_{\text{Sys}}$ of \rref{ex:driving-system} is more complex (and more explicit) than the game model \rref{ex:driving-game}.
The controller now consists of three cases: \rref{eq:driving-sys-forward} says that we can choose velocity within the capabilities of the robot so long as after accelerating for time $\Tvar$ there will be enough time left to brake to stop without hitting the obstacle, \rref{eq:driving-sys-stay} says if the robot is currently stopped it can stay stopped, and \rref{eq:driving-sys-brake}.
As before, \rref{eq:driving-sys-demon} gives Newtonian mechanics for the robot, but of course doesn't have a dual operator because $\alpha_{\textrm{Sys}}$ is not a hybrid game.

Why is $\alpha_{\textrm{Sys}}$ so much longer than \rref{ex:driving-game}?
The key is that in \rref{ex:driving-game}, the controller is existentially quantified in both the safety and liveness theorem: there need only exist some control decision within the constraints of the car that achieves the goal state.
In the language of hybrid systems, we cannot make the controller existentially quantified in the safety theorem without doing the same for the physics (which would simply be wrong).
Because the controller model is universally quantified, it must explicitly write out the sufficient conditions for a safe control decision.
This alone is troublesome: in any verification task, the one thing that cannot be formally proven is that the correct theorem statement was indeed chosen.
For this reason, it is of fundamental importance to achieve the simplest model and simplest theorem statement possible, but using hybrid systems as our modeling language instead of hybrid games imposes a needlessly low ceiling on the simplicity of models.
This is not merely an abstract problem: concretely, experience has shown it is easy to write models in this style with \emph{incomplete controllers}, i.e., where the tests in the controller do not cover all possible program states, leading to safety ``proofs'' that do not cover all possible statess and to systems that would not be safe in practice.
This class of mistakes is simply impossible in the existentially-quantified style of \rref{ex:driving-game}, thus I advocate making this style standard.

The relationship between \rref{ex:driving-system} and \rref{ex:driving-game} is no coincidence:
It is that known that once a winning strategy has been found for a hybrid game, a hybrid system can mechanically been derived.
A similar relation can be expected to hold for \CdGL, and is a key step in end-to-end verification (\rref{ch:proofplex}).


\section{Semantics}
\label{sec:dgl-semantics}
We give the denotational semantics for classical hybrid game logic \dGL, which is divided into a semantics for terms, formulas, and games.
Throughout the semantics, states are written $\omega, \nu, \mu : \allstate$ where the set of states $\allstate$ is in bijection to $\mathbb{R}^n$ for $n = \abs{\allvars \cup \D{\allvars}}$ where $\allvars$ is the set of base variables and $\D{\allvars}$ the set of differential variables.

\begin{definition}[Classical term semantics]\label{def:dgl-sem-term}
The semantics of a term $\tint{\theta}{\om} : \mathbb{R}$ is the value of term $\theta$ in state $\om$, and is defined inductively on $\theta$:
\begin{align*}
  \tint{q}{\om} &= q \\
  \tint{x}{\om} &= \om(x)\\
  \tint{\theta + \eta}{\om} &= \tint{\theta}{\om} + \tint{\eta}{\om}\\
  \tint{\theta \cdot \eta}{\om} &= \tint{\theta}{\om} \cdot \tint{\eta}{\om}\\
  \tint{\der{\theta}}{\om} &= \sum_{x \in \allvars} \frac{\partial \tint{\theta}{\om}}{\partial x} \cdot \omega(\D{x})
\end{align*}
\end{definition}
That is, literals denote themselves, variables $x$ take their meaning from the state $\om,$ sums and products denote the sum and product of the denotatation of their operands respectively, and the differential $\der{\theta}$ denotes the sum of every partial derivative of the denotation of $\theta,$ each scaled by the value of the corresponding differential variable $\D{x}$.
Because every term mentions at most a finite number of variables and the partial with respect to an unmentioned variable is uniformly zero, this sum always has finite support.
An advantage of this semantics is that it is defined in every state while agreeing with the intuitive meaning of ``time derivative of $\tint{\theta}{\om}$'' whenever a differential appears in the postcondition of an ODE.

\begin{definition}[Classical formula semantics]\label{def:dgl-sem-fml}
The semantics of a formula $\phi$ is given by the relation $\fint{\phi}{\om},$ which is defined inductively on $\phi$
\begin{align*}
\fint{\theta \sim \eta}  &= \{\om~|~\tint{\theta}{\om} \sim \tint{\eta}{\om}\} && \text{ for }\sim \in \{\leq,<,=,\neq,>,\geq\}\\
\fint{\neg \phi}         &= \fint{\phi}^C && \\
\fint{\phi \land \psi}   &= \fint{\phi}{\om} \cap \fint{\psi}{\om}\\
\fint{\phi \limply \psi} &= \{\om~|~ \om \in \fint{\phi}{\om} \text{ implies } \om \in \fint{\psi}\}\\
\fint{\phi \lequiv \psi} &= \{\om~|~ \om \in \fint{\phi}{\om} \text{ iff } \om \in \fint{\psi}\}\\
\fint{\lexists{x}{\phi}} &= \{\om~|~ \subst[\om]{x}{r} \in \fint{\phi}\} && \text{ for some }x \in \mathbb{R}\\
\fint{\lforall{x}{\phi}} &= \{\om~|~ \subst[\om]{x}{r} \in \fint{\phi}\} && \text{ for all }x \in \mathbb{R}\\
\fint{\ddiamond{\alpha}{\phi}} &=  \strategyfor[\alpha]{\fint{\phi}}\\
\fint{\dbox{\alpha}{\phi}} &= \dstrategyfor[\alpha]{\fint{\phi}}
\end{align*}
where $\strategyfor[\alpha]{X}$ and $\dstrategyfor[\alpha]{X}$ are the regions from which Angel (or Demon, respectively) has a strategy to reach a region $X$.
\end{definition}

\begin{definition}[Classical game semantics]\label{def:dgl-sem-game}
For any game $\alpha$ and goal region $X \subseteq \allstate$, we inductively define the \emph{winning region} $\strategyfor[\alpha]{X}$ for Angel, i.e., the region from which they have a strategy to enter region $X$ after playing game $\alpha$:
\begin{align*}
\strategyfor[\humod{x}{\theta}]{X} &= \{ \om \in \allstate~|~\subst[\om]{x}{\tint{\theta}{\om}}\} \\
\strategyfor[\prandom{x}]{X}       &= \{ \om \in \allstate~|~\subst[\om]{r}{\tint{\theta}{\om}}\text{ for some }r \in \mathbb{R}\} \\
\strategyfor[\pevolvein{\D{x}=\theta}{\ivr}]{X} &= \{\varphi(0) \in \allstate~|~\varphi(r) \in X\text{ for some }r \in \mathbb{R}_{\geq0}\text{ and (differentiable) }\varphi:[0,r]\to \allstate\\
&\text{ such that }\varphi(\zeta) \in \fint{\ivr}\text{ and } \frac{\partial \varphi(t)(x)}{\partial t}(\zeta) = \tint{\theta}{\varphi(\zeta)}\text{ for all }0 \leq \zeta \leq r \}\\
\strategyfor[\ptest{\phi}]{X}      &= \fint{\phi} \cap X \\
\strategyfor[\alpha \cup \beta]{X} &= \strategyfor[\alpha]{X} \cup \strategyfor[\beta]{X}\\
\strategyfor[\alpha;\beta]{X}      &= \strategyfor[\alpha]{\strategyfor[\beta]{X}}\\
\strategyfor[\prepeat{\alpha}]{X}  &= \bigcap\{Z \subseteq \allstate~|~ X \cup \strategyfor[\alpha]{Z} \subseteq Z\}\\
\strategyfor[\pdual{\alpha}]{X}    &= (\strategyfor[\alpha]{X^C})^C
\end{align*}
Likewise, we define the winning regions $\dstrategyfor[\alpha]{X}$ for \emph{Demon}, which are dual to those for Angel:
\begin{align*}
\dstrategyfor[\humod{x}{\theta}]{X} &= \{ \om \in \allstate~|~\subst[\om]{x}{\tint{\theta}{\om}}\}\\
\dstrategyfor[\prandom{x}]{X}       &= \{ \om \in \allstate~|~\subst[\om]{r}{\tint{\theta}{\om}}\text{ for all }r \in \mathbb{R}\} \\
\dstrategyfor[\pevolvein{\D{x}=\theta}{\ivr}]{X} &= \{\varphi(0) \in \allstate~|~\varphi(r) \in X\text{ for all }r \in \mathbb{R}_{\geq0}\text{ and (differentiable) }\varphi:[0,r]\to\allstate\\
&\text{ such that }\varphi(\zeta) \in \fint{\ivr}\text{ and }\frac{\partial\varphi(t)(x)}{\partial t}(\zeta) = \tint{\theta}{\varphi(\zeta)\text{ for all }0 \leq \zeta \leq r}\}\\
\dstrategyfor[\ptest{\phi}]{X}         &= (\fint{\phi})^C \cup X\\
\dstrategyfor[\alpha\cup\beta]{X}   &= \dstrategyfor[\alpha]{X} \cap \dstrategyfor[\beta]{X}\\
\dstrategyfor[\alpha;\beta]{X}      &=  \dstrategyfor[\alpha]{\dstrategyfor[\beta]{X}}\\
\dstrategyfor[\prepeat{\alpha}]{X}  &= \bigcup\{Z \subseteq \allstate~|~ Z \subseteq X \cap \dstrategyfor[\alpha]{Z}\}\\
\dstrategyfor[\pdual{\alpha}]{X}    &= (\dstrategyfor[\alpha]{X^C})^C
\end{align*}
\end{definition}


%\strategyfor{}{}
%\dstrategyfor{}{}
\section{Proof Calculus}
We recite the proof calculus for \dGL in \rref{fig:dgl-axioms}.
The proof calculus is given as a Hilbert system: axioms are repeatedly applied to recursively decompose hybrid games.
\begin{figure}
  \centering
  \begin{calculuscollections}{\columnwidth}
    \begin{calculus}
\cinferenceRule[dglbox|{$[\cdot]$}]{}
{
\linferenceRule[equiv]{\dbox{\alpha}{\phi}}{\neg\ddiamond{\alpha}{\neg\phi}}
}{}
\cinferenceRule[dglassign|{$\langle:=\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\humod{x}{\theta}}{\phi(x)}}{\phi(\theta)}
}{}
\cinferenceRule[dglrandom|{$\langle:*\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\humod{x}{\theta}}{\phi(x)}}{\lexists{x}{\phi(x)}}
}{}
\cinferenceRule[dglsolve|{$\langle'\rangle$}]{$\D{y}(t)=\theta$}
{
\linferenceRule[equiv]{\ddiamond{\pevolve{\D{x}=\theta}}{\phi}}{\lexists{t{\geq}0}{\ddiamond{\humod{x}{y(t)}}{\phi}}}
}{}
\cinferenceRule[dgltest|{$\langle?\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\ptest{\psi}}{\phi}}{(\phi \land \psi)}
}{}
\cinferenceRule[dglchoice|{$\langle\cup\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\alpha \cup \beta}{\phi}}{\ddiamond{\alpha}{\phi} \lor \ddiamond{\beta}{\phi}}
}{}
\cinferenceRule[dglcompose|{$\langle;\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\alpha;\beta}{\phi}}{\ddiamond{\alpha}{\ddiamond{\beta}{\phi}}}
}{}
\cinferenceRule[dgliter|{$\langle*\rangle$}]{}
{
\linferenceRule[impl]{\phi\lor\ddiamond{\alpha}{\ddiamond{\prepeat{\alpha}}{\phi}}}{\ddiamond{\prepeat{\alpha}}{\phi}}
}{}
\cinferenceRule[dgldual|{$\langle{}^d\rangle$}]{}
{
\linferenceRule[equiv]{\ddiamond{\pdual{\alpha}}{\phi}}{\neg\ddiamond{\alpha}{\neg\phi}}
}{}
\cinferenceRule[dglMon|M]{}
{
\linferenceRule[formula]{\phi\limply\psi}{\ddiamond{\alpha}{\phi}\limply\ddiamond{\alpha}{\psi}}
}{}
\cinferenceRule[dglfixpoint|FP]{}
{
\linferenceRule[formula]{\phi\lor\ddiamond{\alpha}{\psi} \limply \psi}{\ddiamond{\prepeat{\alpha}}{\phi} \to \psi}
}{}
    \end{calculus}
  \end{calculuscollections}
  \caption{Selected axioms for \dGL}
  \label{fig:dgl-axioms}
\end{figure}
Axiom \irref{dglbox} says that the modalities $\ddiamond{\alpha}{\phi}$ and $\dbox{\alpha}{\phi}$ are dual to each other for arbitrary games $\alpha,$ whereas the axiom \irref{dgldual} says game 
$\pdual{\alpha}$ is dual to $\alpha$.
In practical proofs it is often easier to say that the $\pdual{\alpha}$ operation switches players and then plays $\alpha,$ e.g. the derived axiom:
\[\ddiamond{\pdual{\alpha}}{\phi} \lequiv \dbox{\alpha}{\phi}\]
Axiom \irref{dglassign} says assignments can be eliminated by substituting in the postcondition, while \irref{dglrandom} says random assignments can be converted to quantifiers.
Axiom \irref{dglsolve} says that if there exists a function $y(t)$ which solves an ODE, then the ODE can be eliminated by substituting in $y(t)$.
This axiom gives the case where there is no domain constraint because ODEs with domain constraints are derivable from those without them.
The practicality of this axiom depends on the complexity of $y(t)$. 
Especially when $y(t)$ is not a polynomial, it is preferable to reason with \emph{differential invariants}.
Axiom \irref{dgltest} says a test passes when the test condition holds.
Axiom \irref{dglchoice} says Angel chooses which game to play.
Axiom \irref{dglcompose} says game $\alpha;\beta$ is played by playing $\alpha$ first, then $\beta$ in the resulting state.
Axiom \irref{dgliter} says a repetition game can be played by choosing between playing zero rounds or playing some strategy for the first round followed by some strategy for the repetition.
Axiom \irref{dglMon} says hybrid games satisfy monotonicity.
Axiom \irref{dglfixpoint} says repetition $\prepeat{\alpha}$ is a fixpoint of $\alpha$.

\chapter{Completed Work:  Logical Foundations}
\label{ch:logical-foundations}
One of the key components of \emph{foundational} end-to-end synthesis and verification is a bulletproof theoretical foundation.
Logics for verification of hybrid systems and hybrid games are the main formal tool with which we wish to develop correct CPS's, and as such it is doubly important that the logics themselves are developed with the absolute highest degree of certainty in their correctness.
In this chapter, we discuss two completed works by the author.
These works not only attest that an adequate theoretical foundation has been laid for the remaining chapters of the thesis, but also that the author is adequately experienced with developing logical foundations for the proposed works to succeed.

\section{Formalization of \dL in \Isabelle}
\label{sec:isabelle-fml}
When we perform proofs in \dL (or likewise in \dGL), we wish to know that theorem we believe we have proved is indeed true.
This property is called soundness, and paper proofs of soundness are provided in the papers which developed a sequent calculus~\cite{DBLP:journals/jar/Platzer08} and uniform substitution~\cite{DBLP:journals/jar/Platzer17} calculus for \dL, of which the latter is the basis for the implementation of \dL in the theorem prover \KeYmaeraX~\cite{DBLP:conf/cade/FultonMQVP15}.
However, proofs on paper are notoriously susceptible both to
\begin{inparaenum}
  \item errors in human logical reasoning and
  \item even more often, errors by omission: paper proofs often focus on the simplest cases of the proof, providing no evidence that the implementation will continue to be correct in the most difficult cases, exactly when it is most in need of formal assurance.
\end{inparaenum}
Errors of both kinds can be most robustly addressed by formalizing the entire development of the proof system in a general-purpose proof assistant such as Isabelle~\cite{ISABELLE} or Coq~\cite{COQ}, starting from syntax and semantics up to axioms and ultimately a soundness theorem for an entire proof-checking procedure.
Errors of the first kind are the easiest to address: assuming we have not accidentally exploited a bug in the host prover, an erroneous logical step in the mechanized proof will simply be rejected by the prover.
To totally eliminate errors of the second kind requires a more ambitious effort: once we have formalized \dL in another prover, how do we know that we have formalized every important feature and that we have not forgotten one?
After all, if we forget to include a certain feature in our paper proof, we might very easily forget it in a mechanized proof as well, and because this is not a soundness error, we will not catch it simply by attempting to prove soundness in the prover.

Errors of the second kind can be addressed by completing the circle, so to speak: modern theorem provers contain code generation facilities which can be used to synthesize a proof checker from an appropriately-written formal development.
If that proof checker is willing to accept the \dL proofs that are constructed in practice, then there is little question to the exhaustiveness of the development, because no matter whether the paper proof and mechanized proof forgot to account for some feature from the implementation, the implementation itself certainly cannot forget.
While the work involved in bringing a mechanization up to speed with the practical implementation can be significant, there is also a second payoff: the resulting proof checker is guaranteed sound by construction, and can be used as a backup for the standard \KeYmaeraX prover core, effectively removing the core from the trusted computing base.

The author has carried out all of the above developments for the \dL uniform substitution calculus~\cite{DBLP:journals/jar/Platzer17} using Isabelle~\cite{ISABELLE} as a host prover.
The initial formalization is documented in \cite{DBLP:conf/cpp/BohrerRVVP17} and follow-up work in \cite{DBLP:conf/pldi/BohrerTMMP18}.
In all, the formalization includes the syntax, static and dynamic semantics, proof calculus, and soundness theorem for \dL.
The full formalization is $\approx$25,000 lines of Isabelle proof scripts.
In addition, a proof checker was synthesized with the Isabelle code generation mechanism and evaluated by instrumenting \KeYmaeraX to output complete proof terms as it performs a proof.
In \cite{DBLP:conf/pldi/BohrerTMMP18} this proof checker was evaluated on a proof that a velocity-controlled car driving in a straight line is safe under the supervision of a controller generated by \VeriPhy.
The proof term in question is over 100,000 proof steps and is extracted directly from the actual \KeYmaeraX proof, attesting to the exhaustiveness of the formalization.

These results contribute to the overall goals of the thesis in several ways.
Firstly, the proof checker has been used to check the correctness of \dL proofs that end-to-end verified systems as produced throughout this thesis are actually safe, which in effect has bridged the high-level, practical foundation given by \dL with the lower-level foundations of Isabelle.
Secondly, the work requires intimate knowledge of the foundations of \dL, including edge cases which were not addressed in prior works such as systems of (multiple) differential equations and a bound renaming rule, which was in fact unsound in \KeYmaeraX until the bug was discovered by doing the mechanized proof.
Additional features include operations like division, absolute value, and min and max, which are not covered in previous presentations.
This has prepared the author for the foundational work that remains in the proposed thesis chapters.

\subsection{Related Work}
\paragraph{Verification of Theorem Provers}
%
Barras and Werner~\cite{Barras:1997} verified a typechecker for a fragment of Coq in Coq.
Harrison~\cite{Harrison:2006} verified (1) a weaker version of HOL Light's kernel in HOL Light and (2) HOL Light in a stronger variant of HOL Light.
Myreen et al.\ have extended this work, verifying HOL Light in HOL4~\cite{Myreen+Owens+Kumar:2013,Kumar+Arthan+Myreen+Owens:jar:2016} and using their verified compiler CakeML~\cite{Kumar+Myreen+Norrish+Owens:2014} to ensure these guarantees apply at the machine-code level.
Myreen and Davis proved the soundness of the ACL2-like theorem prover Mitawa in HOL4~\cite{Myreen+Davis:itp:2014}.
Anand, Bickford, and Rahli~\cite{Anand+Rahli:2014,Rahli+Bickford:cpp:2016}
proved the relative consistency of Nuprl's type theory~\cite{Constable+al:1986,Allen+al:2006} in Coq with the goal of generating a verified prover core.
%
We share a common goal:
formally verify that theorem provers are correct.
%
However, the underlying theory of our prover greatly differs (\dL intimately deals with programs with differential equations), leading to substantially different proofs (in our case, intricate proofs about real analysis).

\paragraph{Verification of Hybrid Systems}
%
Hybrid systems verification is an actively studied field.
For example, SpaceEx~\cite{Frehse+al:cav:2011} is a model checker that
provides an automated reachability analysis for linear hybrid
automata with a soundness-critical core of about 100\,000 LOC.

We focus on approaches that strive for a justification of the verification technique itself.
Immler~\cite{DBLP:conf/tacas/Immler15} verifies a set-based reachability analysis for ODEs in Isabelle using his differential equations library~\cite{DBLP:conf/itp/ImmlerT16}.
His use of Isabelle makes his analysis more trustworthy than SpaceEx, but it is currently less automated and, more fundamentally, lacks the expressiveness and scalability of \dL.
%
V\"{o}lker~\cite{Volker:2000} defines the semantics of hybrid automata in Isabelle, but no verification techniques.
\'{A}brah\'{a}m-Mumm et al.\
formalized in PVS~\cite{Mumm+Steffen+Hannemann:iceccs:2001} an
automaton-based approach similar to timed automata. 
They implement Floyd's inductive assertion reasoning method.
However, they only check invariants when transitioning between discrete states, as opposed to our differential invariants which hold continuously.
Furthermore, they only support continuous dynamics given as explicit solutions.
Thus they cannot reason by differential invariant, nor can they express systems whose solutions only exist for finite time.
%
StarL~\cite{Lin+Mitra:lctrts:2015} is a framework for programming and simulating Android applications that control robots.
It uses a formalization of timed automata and has a discrete model of space in order to verify distributed applications.
%
Anand and Knepper~\cite{Anand+Knepper:itp:2015} develop the framework ROSCoq based on the Logic of Events for reasoning about distributed CPS in Coq with constructive reals and generating verified controllers.
However, they provide limited support for reasoning about derivatives, requiring extensive manual proofs by the user.
%
VeriDrone by Ricketts et al.~\cite{Ricketts:memcode:2015} is a
framework for verifying hybrid systems in Coq that relies on a
discrete-time temporal logic called RTLA, inspired by
TLA~\cite{Lamport:hybrid:1992}.
%
They prove an analog of DI, but not the other ODE axioms.
They use a combination of a deep and shallow embedding, allowing
arbitrary Coq propositions to be embedded directly into RTLA.
Their embedding reuses Coq's notion of substitution instead of defining its own.
An advantage of uniform substitution is its generality: we can introduce new variable dependencies if they do not clash,
which is necessary for most proof steps.
For example, the V axiom $(p() \limply \dbox{a}{p()})$ can immediately be instantiated to $x=0 \limply \dbox{\D{y}=1}{x=0}$ 
because $\boundvars{\D{y}=1} \cap \freevars{x=0} = \emptyset$, introducing dependencies on $x$ and $y$ and $\D{y}$.
The permitted syntactic occurrence patterns for uniform substitutions are easily decidable, thereby enabling automatic clash detection and sound generalizations of axiom shapes.
That needs the syntactic exposition of a deep embedding, though.

\section{Definite Descriptions in dL}
\label{sec:definite-description}
While \rref{sec:isabelle-fml} addresses the gap between theory presentations of \dL and its usage in the theorem \KeYmaeraX, it does so in a somewhat brute force way: manually formalizing every extension that has been added to the \KeYmaeraX core over time.
This is unmaintainable as the number of extensions grows, and moreover we would prefer a definition mechanism where new constructs can be defined without increasing the size of the core.
That is to say, one of the lessons learned from the work described in \rref{sec:isabelle-fml} is that a more maintainable way to put a theorem prover on bulletproof foundations is to work from both ends at once, not just formalizing the proof calculus in great detail but also redesigning the proof calculus to be more general in the first place, so that what were once a grabbag of extensions are now elegantly described in the core language.

Recent work by the author~\cite{hilbert-epsilons} does exactly that by introducing the logic \dLi which extends \dL with a definite description construct $\meps{x}{\phi(x)}$ meaning the unique $x$ (if a unique one exists) such that $\phi(x)$ is true.
The underlying reason for this extension is that an unending array of extensions to \dL, both past and future, are simply new term constructs which do not fall within the restrictive polynomial language of traditional \dL.
Because the formula language of \dL is relatively unrestricted, containing all of first-order real arithmetic and dynamic modalities, one can achieve the desired power in the term language through the definite description construct, which effectively imports the powers of the formula language into the term language.

This work is of relevance to the overall thesis in two ways.
First, it justifies the choice to focus primarily on the core polynomial term language in the development of \CdGL, as the foundations of additional terms (albeit a classical foundation) have now been studied in detail.
Moreover, it again demonstrates the author's ability to work with the foundations of the logic: whereas the work in \rref{sec:isabelle-fml} focused on a painstakingly detailed reproduction of a proof calculus which had largely already been developed, this work required a wholesale overhaul of the semantics underlying \dL, much in the way an extension from classical \dGL to constructive \CdGL will require another semantic overhaul.

It is perhaps surprising that \dLi requires a new semantics in the first place.
The simplicity of \dL terms is exploited in several ways throughout the theory of \dL, the main properties being that terms are defined everywhere and that they are everywhere infinitely differentiable and locally Lipschitz-continuous, meaning that all differential equations expressible in \dL have guaranteed uniqueness and existence of solutions by the Picard-Lindel\"of theorem.
The simple addition of definite descriptions invalidates all of these assumptions.
To deal with this generalization, the term semantics of \dLi are made partial and the formula semantics are recast a 3-valued {\L}ukasiewicz semantics.
All axioms and proof rules of \dL are then revised so that hold valid over the 3-valued semantics of \dLi and are proven sound again with respect to the new semantics.
The \dLi calculus maintains as much generality as reasonably possible, so that for example it is possible to prove properties even about systems which do not satisfy uniqueness of solutions.

The theoretical developments for \dLi culminate in soundness and equi-expressiveness proofs with respect to \dL, which very much the same kind of results we aim to achieve for the \CdGL theory.

\subsection{Related Work}
Formalizations of \KeYmaeraX~\cite{DBLP:conf/pldi/BohrerTMMP18}, Coq~\cite{DBLP:journals/jfrea/Barras10}, and NuPRL~\cite{DBLP:conf/itp/AnandR14} all omit or simplify whichever practical features are most theoretically challenging for their specific logic: discontinuous and partial terms in \KeYmaeraX, termination-checking in Coq, or context management in NuPRL.
When formalizations of theorem provers \emph{do} succeed in reflecting the implementation~\cite{DBLP:journals/jar/KumarAMO16}, they owe a credit to the generality of the underlying theory: it is much more feasible to formalize a general base theory than to formalize ad-hoc extensions as they arise.
Our calculus, as with modern implementations~\cite{DBLP:conf/cade/FultonMQVP15} and machine-checked correctness proofs~\cite{DBLP:conf/cpp/BohrerRVVP17} for \dL, is based on uniform substitution~\cite[\S35,\S40]{Church:1956}: symbols ranging over predicates, programs, etc. are explicitly represented in the syntax.
This improves the ease with which \dLi can be implemented and its soundness proof checked mechanically in future work.
We show that in contrast to \dL, \dLi can directly represent a hybrid system featuring such ODEs, which we base on Hubbard's leaky bucket~\cite[\S4.2]{Hubbard}.
Then the water drains out the cooler at a rate proportional to the square root of the current volume  by Torricelli's Law~\cite{driver1998torricelli}, or rate 0 if the valve is closed.
In contrast to the indexed variables of \QdL~\cite{DBLP:journals/lmcs/Platzer12b}, they are equipped with an induction operator, making it easier to write sophisticated computations.
\irref{DI} is the \emph{differential induction}~\cite{DBLP:journals/logcom/Platzer10}
Quantifier elimination rule \irref{QEeps} says first-order arithmetic, a fragment, is decidable~\cite{tarski1998decision}.

\chapter{Completed Work:  End-to-End Robot Verification}
\label{ch:end-to-end-v}
This chapter describes  works by the author and collaborators which introduce a general approach that enables end-to-end verification while maintaining solid logical foundations and then apply the approach in a case study on the verification of free driving for 2D Dubins-like ground robots.
These works already constitute a major cohmponent of the goals of the thesis: what remains in the proposed work is to extend this beyond only verification and monitor synthesis to include controller synthesis while maintaining rigorous foundations.
We begin by motivating end-to-end verification and defining it, then give our approach and case study results in greater detail.

\section{\VeriPhy Pipeline for End-to-End Verification}
\label{sec:veriphy}
As discussed in the introduction (\rref{ch:introduction}), end-to-end verification of CPS is in part motivated by the same factors as CPS in general: CPS such as self driving cars and robots are often safety critical because of their proximity to humans, and formal methods are a powerful tool for insuring that these critical systems are in fact safe enough to be trusted around humans. Because CPS are only growing more and more popular over time, any work that addresses their safety is poised to increase in impact, as well.

To fully motivate end-to-end verification, however, one must assess the key question of what it even \emph{means} to verify the safety of a CPS.
When we first learn how to prove theorems in math class, we may not always know how to do the proof, or we may not always know whether a proof has sufficient detail, but it is almosts always clear \emph{what} we are trying to do, because the theorems we prove come from a longstanding mathematical tradition with an established library of well-defined and well-understood mathematical terms: there is little debate today what it means to be a real number, nor what it means to be a group.
As soon as we enter the realm of even discrete programs, this changes entirely. Not only might programs be written in an endless and ever changing array of programming languages, but it is no longer clear what we want to prove about a program. On one end of the spectrum we might simply prove that a program satisfies a simple property like termination or absence of segfaults and call it a day, while at the other end of the spectrum we might develop a full functional specification and prove that the entire specification is met. In practice, we often want something in between, because the more expressive a program property is, typically the harder it is to prove. All of this is also compounded by the nature of software: while the real numbers change only occasionally (with the advent of fields like nonstandard analysis or constructive analysis \texttt{;-)}), real software can change hundreds of times per day. Many interesting programs can only be proved with the intervention of humans, and it is a total nonstarter to ask a human to perform the verification of a complex program hundreds of times per day.

These issues only grow more pernicious in CPS, where physics enters the mix as well.
It is equally true in CPS that code evolves and should not need to be constantly reverified by hand.
It is even more true that it becomes difficult to nail down a proper specification: not only are there a variety of possible properties to prove, but because CPS's are fundamentally constrained by the laws of physics, the most general properties are often not provable because they are \emph{simply false}: A car cannot avoid collision if it is an inch behind an intruder vehicle that slams on its breaks, nor can a car avoid collisions if its sensors exhibit an arbitrarily large error.
Even once a specification has been identified, proving it is now more difficult because the verification of CPS spans several subspecialties of verification which require diverse reasoning principles.
At the controls level (which is the focus of this thesis), CPS verification must reason seamlessly about both discrete control code and continuous dynamical systems (differential equations) that describe the kinematics of the system. Furthermore, these arguments are by far easiest to make at a high level of abstraction where controls are nondeterministc, variables are real-valued, and time evolves continuously (which also provides the strongest results), yet we expect results to hold of a low-level implementation where control is deterministic, variables have finite arithmetic precision, and a controller can only sense every so often.
This is without even considering stochastic, distributed, and adversial dynamics, let alone vulnerabilities in the network stack.

All of this goes to say, that in seeking end-to-end verification of CPS, in seeking the strongest possible results about CPS as they exist in actual implementation and not merely in mathematical models, \emph{identifying the right question is half the battle}.
As soon as we seek to nail down a definition of end-to-end verification, we will realize this is in fact a moving target.
It is unfeasible to summarily verify every desirable property of every sensor, actuator, and controller, simply because a cornucopia of such components exist, all of which are in constant flux: 
If today we prove that today's controller is correct, Murphy's Law dictates that a superior controller is released for purchase tomorrow.
With all the considerations in mind, it is less essential to ensure that every conceivable component of the CPS has been verified in full, but rather to devise and carry out an approach where:
\begin{enumerate}
\item \label{item:evolution} Control software is free to evolve without reverification
\item \label{item:end-to-end} Safety of the system as it is implemented can be tied formally to a rigorous theoretical foundation
\item \label{item:high-level} When a human is required to perform verification, they are allowed to do so at a high level (hybrid systems) with their results automatically transported to implementation level
\item \label{item:open-ended} The architecture is open-ended enough that one could plausibly verify those components whose correctness is presently assumed.
\end{enumerate}
While it is specifically because of requirement \ref{item:end-to-end} that we call our work \emph{end-to-end}, the author believes that all of the above are essential for verification to be applicable in realistic systems while still claiming full generality and full rigor.
Our approach, which we will refer to as the \VeriPhy approach (as it is implemented in our tool \VeriPhy), achieves all of the above, as we will now describe.

Requirement \ref{item:evolution} effectively requires a significant \emph{runtime verification} component, where the safety of control decisions is checked online, and potentially-unsafe decisions are replaced by provably safe ones.
Requirements \ref{item:end-to-end} and \ref{item:high-level} entail an equally significant hybrid systems verification component.
Requirement \ref{item:open-ended}, arguably the easiest to satisfy, can be satisfied by providing a clean interface between those components which are proven correct and those which are trusted.

With all these requirements in mind, the \VeriPhy approach starts with modeling and verifying a hybrid system in \dL using \KeYmaeraX.
The \VeriPhy tool then synthesizes an \emph{executable correct-by-construction monitor}  as follows:
\begin{itemize}
\item The \ModelPlex tool is invoked to synthesize a \emph{control monitor formula} and \emph{plant monitor formula}, which determine whether control decisions and physical evolutions respectively satisfy the assumptions made of them by the model
\item The monitors are combined with a proven-safe \emph{fallback controller} (which is provided in the input model) to form a \emph{sandbox controller}.
   The sandbox controller applies control decisions provided from an external source (henceforth called the \emph{external controller}) whenever those decisions satisfy the controller monitor,
   and otherwise invokes the fallback controller
\item The sandbox controller is automatically proven safe in \KeYmaeraX, in part by reusing the contents of the hybrid system safety proof
\item The sandbox safety proof is optionally rechecked in the verified proof checker (\rref{sec:isabelle-fml}) to eliminate the \KeYmaeraX core from the trusted base
\item The exact real arithmetic of hybrid systems is replaced with conservative fixed-point interval arithmetic, which is formally proven in Isabelle to be sound
\item The resulting program is proven equivalent to a CakeML program (in HOL4)
\item The equivalent CakeML program is compiled to machine code in a verified compiler
\item The machine code is linked against the users' implementation of sensors and actuators
\end{itemize}
Each step is provably a refinement of the step before it, culminating that any execution of the machine-code running on the actual CPS is corresponds to some execution of the source hybrid system.
Because all executions of the source hybrid system were proven safe already, then the execution of the actual CPS is also known to be safe.
Notably, the sensors and actuator drivers, which are provided by the users of \VeriPhy, are trusted.
Yet requirement \label{item:open-ended} is satisfied: the sensors and actuators (correspond directly to the sensor and actuator variables in the hybrid system model) are isolated in a small foreign-function interface which has a simple specification in HOL4.
It is not unrealistic to suggest that one day, we could verify sensors and actuators against these specifications in HOL4, at least given some weaker assumptions.

\subsection{Related Work}
\paragraph{CPS Verification.}
Differential dynamic logic~\cite{DBLP:journals/jar/Platzer08,DBLP:conf/lics/Platzer12a}, as implemented in \KeYmaeraX~\cite{DBLP:conf/cade/FultonMQVP15}, has been successfully applied in a number of case studies~\cite{DBLP:conf/cade/Platzer16}.
Soundness of its core calculus~\cite{DBLP:journals/jar/Platzer17} has been cross-verified in \Isabelle and Coq~\cite{DBLP:conf/cpp/BohrerRVVP17}, which this work extended to a verified proof-term checker in \Isabelle.

General-purpose logics have also been used for CPS proofs.
ROSCoq~\cite{DBLP:conf/itp/AnandK15} and VeriDrone~\cite{DBLP:conf/cpsweek/MalechaRAL16} allow CPS specification, implementation, verification, and code generation in Coq.
However, they do not synthesize and automatically verify monitors nor is their machine code verified.

We provide greater automation: the user need only verify the source model and can exploit \KeYmaeraX automation.
On the other hand, Coq provides ROSCoq and VeriDrone with greater freedom in models and verified controllers, which \VeriPhy has only in the untrusted controller.

Hybrid model checking~\cite{DBLP:conf/cav/FrehseGDCRLRGDM11,DBLP:conf/cav/ChenAS13,DBLP:conf/tacas/DuggiralaMVP15} typically has a large trusted base and sacrifices expressiveness to increase automation, being restricted, e.g., to linear differential equations, bounded-time safety, or bounded state-space verification.

\paragraph{Toolchains.}
High-Assurance SPIRAL \cite{DBLP:journals/csm/FranchettiLMMGPPKMFJPV17} translates ModelPlex monitors to x64 machine code.
Its verification is not end-to-end: because it only treats hybrid systems syntactically, it cannot verify across semantic gaps, e.g., in interval arithmetic and compilation.
Its optimizer employs features such as SIMD instructions that would make verified compilation especially challenging.
%
Ivory \cite{DBLP:conf/haskell/ElliottPWHBSSL15} and Tower \cite{DBLP:conf/plpv/PikeHBEDL14} are used to ensure high-assurance embedded software is memory safe, but do not verify the functional correctness of cyber nor physical behavior.

\paragraph{Verified Compilation.}
We use the CakeML~\cite{DBLP:conf/icfp/TanMKFON16} verified ML compiler and its associated verification tools~\cite{DBLP:conf/icfp/MyreenO12,DBLP:conf/esop/GueneauMKN17}.
CakeML is higher-level than other languages such as Clight with verified compilers such as floating-point CompCert~\cite{DBLP:conf/arith/BoldoJLM13}.
This makes the verification of sandbox implementation in CakeML against hybrid systems semantics painless.
We chose this over, e.g., translation validation with unverified compilers~\cite{DBLP:conf/pldi/SewellMK13} since translation validation can be brittle.

Lustre~\cite{DBLP:journals/tse/HalbwachsLR92} is a CPS-centric language with a verified compiler, V\'{e}lus~\cite{DBLP:conf/pldi/BourkeBDLPR17}.
Writing and compiling a Lustre controller provides no end-to-end guarantees because physical modeling and verification are left unanswered.
It could be used as a code generation target, but this would be a detour because the Lustre language differs greatly from hybrid programs.
%Related work on compilation after Isabelle
%Examples of these include CompCert~\cite{DBLP:journals/cacm/Leroy09}, a verified C compiler, and Cogent~\cite{DBLP:conf/asplos/AmaniHCRCOBNLST16}, a linearly typed, total functional language with proof-producing compilation to C.

\paragraph{Machine Arithmetic Verification.}
Machine arithmetic correctness is a major \VeriPhy component.
We verify arithmetic soundness foundationally.
This is an active research area with libraries available in HOL Light~\cite{DBLP:conf/sfm/Harrison06}, Coq~\cite{DBLP:conf/arith/BoldoM10,DBLP:conf/tphol/DaumasRT01,DBLP:conf/mkm/BoldoFM09,DBLP:journals/iandc/Melquiond12}, \Isabelle~\cite{DBLP:journals/afp/Yu13}, etc.
Of these, only PFF in Coq~\cite{DBLP:conf/tphol/DaumasRT01} provides the qualitative rounding correctness results we need, so we prove them in \Isabelle using the seL4~\cite{DBLP:journals/cacm/KleinAEHCDEEKNSTW10} machine word library.
We chose \Isabelle and HOL4 over Coq because their combination of cutting-edge analysis libraries~\cite{DBLP:conf/itp/ImmlerT16}, mature formalization of $\dL$~\cite{DBLP:conf/cpp/BohrerRVVP17}, proof-producing code extraction~\cite{DBLP:conf/icfp/MyreenO12}, and classical foundations positions them well for our end-to-end pipeline.
Static analysis of arithmetic for hybrid systems has been studied~\cite{DBLP:conf/emsoft/MartinezMST10,DBLP:conf/emsoft/MajumdarSZ12,DBLP:conf/cav/BouissouGPTV09}, but without foundational safety proofs for general dynamics.

\section{Case Study: End-to-End Ground Robotics Verification}
\label{sec:ground-robotics}
\newcommand{\xgivar}{\textsf{xi}}
\newcommand{\ygivar}{\textsf{yi}}
\newcommand{\xgvar}{\textsf{x}}
\newcommand{\ygvar}{\textsf{y}}
\newcommand{\xcvar}{\textsf{xc}}
\newcommand{\ycvar}{\textsf{yc}}
\newcommand{\ytvar}{\textsf{yt}}
%\newcommand{\xivar}{\textsf{xi}}
%\newcommand{\yivar}{\textsf{yi}}
%\newcommand{\xvar}{\textsf{x}}
\newcommand{\yvar}{\textsf{y}}
\newcommand{\wvar}{\textsf{w}}
\newcommand{\dxvar}{\textsf{dx}}
\newcommand{\dyvar}{\textsf{dy}}
\newcommand{\dxivar}{\textsf{dxi}}
\newcommand{\dyivar}{\textsf{dyi}}
\newcommand{\dxgvar}{\textsf{dxg}}
\newcommand{\dygvar}{\textsf{dyg}}
\newcommand{\kvar}{\textsf{k}}
\newcommand{\tvar}{\textsf{t}}
\newcommand{\vivar}{\textsf{vi}}
\newcommand{\vlvar}{\textsf{vl}}
\newcommand{\vhvar}{\textsf{vh}}
\newcommand{\ctrl}{\textsf{ctrl}\xspace}
\newcommand{\ctrlliv}{\ctrl_{\text{a}}}
\newcommand{\plant}{\textsf{plant}\xspace}
\newcommand{\psimp}{\ensuremath{\alpha_{SV}}\xspace}
\newcommand{\exctrl}{\textsf{ctrl}_{SV}\xspace}
\newcommand{\pdrive}{\textsf{go}\xspace}
\newcommand{\pstop}{\textsf{stop}\xspace}
\newcommand{\explant}{\textsf{plant}_{SV}\xspace}
\newcommand{\lnorm}[1]{{{\norm{#1}}_{\infty}}}
\newcommand{\enorm}[1]{\norm{#1}}
\newcommand{\linv}{J}

While the previous section describes \VeriPhy in general (both as a methodology and as a tool), we certainly cannot claim that the \VeriPhy tool enables end-to-end verification of realistic CPS implementations until we have actually applied \VeriPhy to such a system.
This section validates that claim by providing a full \VeriPhy case study including \dL models, proofs, a simulation which employs the sandbox controller generated by \VeriPhy, and test environments for the simulation.
For our case study, we chose 2D ground robots which follow Dubins-like arcs and lines and which enforce user-supplied speed limits as they do so.
This choice is motivated by the fact that a huge variety of ground robots move by following such paths, yet the paths themselves are reasonably simple, and at the same time speed limits are incredibly useful for example in autonomous cars which intend to obey the law or in robots with high centers of mass which wish not to flip over when navigating curves.

\subsection{Model}
This section introduces our 2D robot model in \dL.
The Dubins dynamics are bloated by a tolerance, which accounts for imperfect actuation and for discrepancies between the Dubins dynamics and real dynamics of the implementation.
That is, because realistic robots never follow a path perfectly, we will bloat each arc to an annulus section which is more easily followed.
Each waypoint is specified by coordinates $(\xgvar,\ygvar)$, a curvature $\kvar$, and a speed limit $[\vlvar,\vhvar]$ for the robot's velocity $\vvar$ which is to be met by the time the waypoint is reached.
The curvature parameter yields circular arcs (when $\kvar \not= 0$) and lines (when $\kvar=0$) as primitives.

Our hybrid program $\alpha$ is a time-triggered \emph{control-plant} loop: $\alpha\equiv(\ctrl;\plant)^*$.
We use \emph{relative} coordinates: the robot's position is always the origin, from which perspective the waypoint ``moves toward'' the vehicle.
This simplifies proofs (fewer variables) and implementation (real sensors and actuators are vehicle-centric).
The $\plant$ is an ODE describing the robot kinematics:
\begin{align*}
\plant\equiv \{&\D{\xgvar}=-\vvar~\kvar~\ygvar,~\D{\ygvar}=\vvar~(\kvar~\xgvar-1),~\D{\vvar}=\avar,~\D{\tvar}=1 \\
 &\&~\tvar\leq \Tvar ~\land~ \vvar \geq 0\} %  ~\land~ yg > 0
\end{align*}
Here, $\avar$ is an input from the controller describing the acceleration with which the robot is to follow the arc of curvature $\kvar$ to waypoint $(\xgvar,\ygvar)$.
In the equations for $\D{\xgvar},\D{\ygvar}$: \begin{inparaenum}[\it i)]\item The $\vvar$ factor models $(\xgvar,\ygvar)$ moving at linear velocity $\vvar$, \item The $\kvar,\xgvar,\ygvar$\ factors model circular motion with curvature $\kvar$, with $\kvar > 0$ corresponding to counter-clockwise rotation of the waypoint, $\kvar < 0$ to clockwise rotation, and $\kvar = 0$ to straight-line motion, \item The additional $-1$ term in the $\D{\ygvar}$ equation shifts the center of rotation to $\left(\frac{1}{\kvar},0\right)$.
\end{inparaenum}
The equations $\D{\vvar}=\avar$ and $\D{\tvar}=1$ make acceleration the derivative of velocity and $\tvar$ stand for current time.
The domain constraint $\tvar \leq \Tvar \land \vvar \geq 0$ says that the duration of one control cycle shall never exceed a timestep parameter $\Tvar > 0$ representing the maximum delay between control cycles and that the robot never drives in reverse.

The controller's task is to compute an acceleration $\avar$ which slows down (or speeds up) soon enough that the speed limit $\vvar \in [\vlvar,\vhvar]$ is ensured \emph{by the time} the robot reaches the goal.
The controller is written:
\newcommand{\admiss}{\textsf{Go}} % (\xgvar,\ygvar,\vlvar,\vhvar,\kvar,\avar)
\newcommand{\planreq}{\textsf{Feas}} % (\xgvar,\ygvar,\vlvar,\vhvar,\kvar,\avar)
\newcommand{\veps}{\varepsilon}
\newcommand{\annul}{\textsf{Ann}\xspace}
\newcommand{\adjustSpeedDist}{\delta_\mathsf{Lim}\xspace}
\newcommand{\controllableGoalDist}{\mathsf{Lim}}
%\annul &\equiv
%\left|\frac{ v~\xgivar~\left(\xgvar^2 +\ygvar^2\right)}{\xgivar^2+\ygivar^2} - v~\xgvar\right| \leq  \veps^2 \tag{a}\\
%\land&~\ygvar>0 \tag{b} \\
%\land&~\frac{2~\xgivar~(\xgvar~\ygivar-\xgivar~\ygvar)}{\xgivar^2+\ygivar^2}+\ygvar \leq \ygivar \tag{c}
%\land&~\ygvar>0 \tag{c}
{\small\begin{align*}
\annul &~\mequiv~\abs{\kvar}\veps \leq 1 \land \Big|\frac{ \kvar~\left(\xgvar^2 +\ygvar^2 - \veps^2 \right)}{2} - \xgvar\Big| < \veps\\
%ygvar>0 included for consistency with the proof (it is also monitored to satisfy our liveness assumption)
\label{eq:planReq} \planreq &~\equiv \annul \land \ygvar{>}0 \land 0{\leq}\vlvar{<}\vhvar \land \Avar\Tvar{\leq}\vhvar-\vlvar \land \Bvar\Tvar{\leq}\vhvar-\vlvar\\
\ctrl &~\equiv~ \prandom{(\xgvar,\ygvar)};\,\prandom{[\vlvar,\vhvar]};\,\prandom{\kvar};\,\ptest{\planreq};\,\underbrace{\prandom{\avar};\,\ptest{\admiss}}_{\ctrlliv}\\
      ~&\hspace*{-5pt}\admiss \equiv-\Bvar\leq \avar \leq\Avar \land \vvar + \avar\Tvar \geq 0 \\
\land & \Bigl(\vvar \leq \vhvar \land \vvar+\avar\Tvar\leq \vhvar ~\lor \\
      & (1+\abs{\kvar}\veps)^2 \Bigl(\vvar\Tvar+\frac{\avar}{2}\Tvar^2 + \frac{(\vvar{+}\avar\Tvar)^2-\vhvar^2}{2\Bvar}\Bigr) + \veps{\leq}\lnorm{(\xgvar,\ygvar)} \Bigr)\\
\land & \Bigl(\vlvar \leq \vvar \land \vlvar \leq \vvar+\avar\Tvar ~\lor\\
      & (1+\abs{\kvar}\veps)^2 \Bigl(\vvar\Tvar+\frac{\avar}{2}\Tvar^2 + \frac{\vlvar^2 - (\vvar{+}\avar\Tvar)^2}{2\Avar}\Bigr) + \veps{\leq}\lnorm{(\xgvar,\ygvar)}\Bigr)\end{align*}}
\noindent where the plan assignment $\prandom{(\xgvar,\ygvar)}$ chooses the next 2D waypoint, the assignment $\prandom{[\vlvar,\vhvar]}$ chooses the speed limit interval, and $\prandom{\kvar}$ chooses any curvature.
The \emph{feasibility} test $\ptest{\planreq}$ determines whether or not the chosen waypoint, speed limit, and curvature are \emph{physically} attainable in the current state under the $\plant$ dynamics (e.g., it checks that there is enough remaining distance to get within the speed limit interval).
We also simplify plans so all waypoints satisfy $\ygvar > 0,$ but subdividing any violating paths automatically.
This simplifies the feedback controller and proofs.
The abbreviation $\ctrlliv$ names just the control code responsible for deciding \emph{how} the waypoint is followed rather than \emph{which} waypoint is followed.


In $\planreq,$ formula $\annul$ says we are within the \emph{annulus section} (\rref{fig:circlestaging}) ending at the waypoint $(\xgvar, \ygvar)$ with the specified curvature $\kvar$ and width $\epsilon$.
A larger choice of $\veps$ yields more error tolerance in the sensed position and followed curvature at the cost of an enlarged goal region.
Formula $\annul$ also contains a simplifying assumption that the radius of the annulus is at least $\epsilon$.
$\planreq$ also says the speed limits are assumed distinct and large enough to not be crossed in one control cycle.

\begin{figure}[h!]
\centering
\includegraphics[width=0.3\textwidth]{graphics/fig-ode3.pdf}
\caption{Annular section through the (blue) waypoint $(2.5,2.5)$. Trajectories from the displaced green and red waypoints with slightly different curvatures remain within the annulus.}\label{fig:circlestaging}
\label{fig:circlestaging}
\end{figure}

The \emph{admissibility} test $\ptest{\admiss}$ checks that the chosen $\avar$ will take the robot to its goal with a safe speed limit, by \emph{predicting future motion} of the robot.
We illustrate this with the upper bound conditions.
The bound will be satisfiable after one cycle if either the chosen acceleration $\avar$ already maintains speed limit bounds ($\vvar \leq \vhvar \land \vvar{+}\avar\Tvar \leq \vhvar$) or when there is enough distance left to restore the limits before reaching the goal.
For straight line motion ($\kvar=0$), the required distance can simply be found by integrating acceleration and speed:
\begin{equation*}
\underbrace{\vvar\Tvar+\frac{\avar}{2}\Tvar^2}_{\mathclap{\text{distance in time}~\Tvar}} + \frac{(\overbrace{\vvar{+}\avar\Tvar}^{\mathclap{\text{speed in time}~\Tvar}})^2-\vhvar^2}{2\Bvar} + \veps \leq \lnorm{(\xgvar,\ygvar)}
\end{equation*}
where $\avar \in [-\Bvar,\Avar]$.
The extra factor of $(1 + \abs{k}\veps)^2$ for curved motion accounts for the fact that an arc along the inner side of the annulus is shorter than one along the outside (\rref{fig:circlestaging}).

\subsection{Proofs}
In contrast to prior works~\cite{DBLP:journals/ijrr/MitschGVP17}, this work contains both safety and liveness proofs for the full 2D dynamics.\footnote{In the interest of full disclosure, the modeling and proof sections were largely performed by collaborators in this work. The present author provided simulations, writing, integration with \VeriPhy and advice on the modeling and proof tasks. In contrast, the modeling and proofs in the proposed work are to be performed by the present author}.
In many ways, this lays a foundation for the proposed work because a proof of winnability for a hybrid game will contain both liveness-like (strategy for our player) and safety-like components (strategy for the opponent).
Taken at surface value, the safety theorem says that speed limits are obeyed whenever the robot reaches a waypoint
\begin{theorem}[Safety]
\label{thm:safe}
The following \dL formula is valid:
\begin{align*}
&\Avar{>}0 \land \Bvar{>}0 \land \Tvar{>}0 \land \veps{>}0 \land \linv \limply \\
&\dbox{\prepeat{(\ctrl;\plant)}}{\big(\enorm{(\xgvar,\ygvar)} \leq \veps \limply \vvar \in [\vlvar,\vhvar]\big)}
\end{align*}
\end{theorem}
Taken more broadly, safety also includes the fact that the robot always remains within an $\epsilon$ annulus around the arc leading to the waypoint, which arises as part of the loop invariant used to prove \rref{thm:safe}.
The first four assumptions ($\Avar{>}0 \land \dots \land \veps{>}0$) are basic sign conditions on the symbolic constants used in the model. 
The final assumption, $\linv$, is the loop invariant.
The full definition of $\linv$ is deferred to \rref{sec:robostage}.
We write $\enorm{(\xgvar,\ygvar)}$ for the Euclidean norm $\sqrt{\xgvar^2 + \ygvar^2}$ and consider the robot ``close enough'' to the waypoint when $\enorm{(\xgvar,\ygvar)} \leq \veps$ for our chosen goal size $\veps$.
%The theorem states that no matter which (admissible) control decisions are made, whenever the vehicle is in the goal region of size $\veps$, it obeys the speed limit $\vvar \in [\vlvar,\vhvar]$.
While this captures the desired notion of safety, it does not prove that the robot can actually reach the goal, which is a \emph{liveness} property:
\begin{theorem}[Liveness]
\label{thm:liveness}
The following \dL formula is valid:
\begin{align*}
&\Avar{>}0 \land \Bvar{>}0 \land \Tvar{>}0 \land \veps{>}0 \land \linv \limply \\
&\dbox{\prepeat{(\ctrl;\plant)}}{\Big( \vvar{>}0 \land \yvar{>}0 \limply }\\
&\ \ \ \ \ddiamond{\prepeat{(\ctrlliv;\plant)}}{\big(\enorm{(\xgvar,\ygvar)} \leq \veps~\land~\vvar \in [\vlvar,\vhvar]\big)} \Big)
\end{align*}
%where $\ctrlliv$, as before, controls $\avar$.
\end{theorem}

This theorem has the same assumptions as \rref{thm:safe}. 
It says that no matter how long the robot has been running ($\dbox{\prepeat{(\ctrl;\plant)}}{}$), 
then if some simplifying assumptions still hold ($\vvar{>}0\land\yvar{>}0$) 
the controller can be continually run ($\ddiamond{\prepeat{(\ctrlliv;\plant)}}{}$) with admissible acceleration choices $(\ctrlliv)$ to reach the present goal \((\enorm{(\xgvar,\ygvar)} \leq \veps)\) within the desired speed limits \((\vvar \in [\vlvar,\vhvar])\).
The simplifying assumptions $\vvar{>}0\land\yvar{>}0$ say the robot is still moving forward and the waypoint is still in the upper half-plane, i.e., we have not run \emph{past} the waypoint.

The liveness theorem is invaluable because it validates the \dL model: if it were not even possible at the level of the \dL model to achieve liveness, then it would be absolutely impossible at the level of implementation to achieve liveness.
Because it is undesirable to change the model drastically once implementation has started (i.e., delays in modeling and verification will trickle down to greater delays in implementation and testing), it is valuable to have this confidence in the model before implementation even begins.
Moreover, the combination of safety and liveness gives us confidence that a game generalization of this proof will succeed, as the main components of a game proof are liveness and safety reasoning.

\subsection{Simulations} 
In order to claim that \VeriPhy has been evaluated on a realistic system implementation, it is essential that we choose a simulation platform that is reasonably faithful to the physics of actual autonomous cars.
For this purpose we chose the AirSim~\cite{shah2018airsim} simulator, which is notable for combining top-notch visuals, reasonable physics, and an open-source code base that is particularly friendly to modification.
While some aspects of the underlying physics engine are closed-source, many aspects are open to customization, including for example the details of wheels and suspensions, and the center of gravity.
For this reason and because AirSim has been successfully applied in dozens of projects, we have good reason to believe its physics model is more faithful than any purpose-built one-off simulation would be.

The author implemented several bang-bang and PD (proportional-derivative) controllers in AirSim as well as several environments which provided a variety of driving conditions with turns of different radii.
The environments and the evaluation results are summarized below:
\begin{figure*}[tb]
\centering
\begin{minipage}[b]{2in}\centering
\begin{tikzpicture}[->,draw,thick,yscale=0.9,inner sep=1pt]
\tikzstyle{arc}=[circle,draw]
\tikzstyle{line}=[rectangle,draw]
\tikzstyle{way}=[diamond,draw]
\node[way] (a) at (1,3) {A};
\node[way] (b) at (4,3) {B};
\node[way] (c) at (4.75,1.5) {C};
\node[way] (d) at (3.25,1.5) {D};
\node[way] (e) at (4,0) {E};
\node[way] (f) at (1,0) {F};
\node[way] (g) at (1.75,0.90) {G};
\node[way] (h) at (0.25,0.90) {H};
\node[way] (i) at (1.75,2.15) {I};
\node[way] (j) at (0.25,2.15) {J};
\draw (a) -> (b);
\draw (b) edge [bend left] node {} (c);
\draw (b) edge [bend right] node {} (d);
\draw (c) edge [bend left]node {} (e);
\draw (d) edge [bend right] node {} (e);
\draw (e) -> (f);
\draw (f) edge [bend left]  node {} (h);
\draw (f) edge [bend right] node {} (g);
\draw (h) -> (j);
\draw (g) -> (i);
\draw (j) edge [bend left]  node {} (a);
\draw (i) edge [bend right] node {} (a);
\end{tikzpicture}
\subcaption{Example mission}\label{fig:patrol-mission-plan}\end{minipage}
\begin{minipage}[b]{1.51in}\centering
\includegraphics[width=1.51in,clip,trim=0 0 0 50]{graphics/airsim.png}
\subcaption{Simulator}\label{fig:simulator}\end{minipage}
%\begin{minipage}[b]{.3\linewidth}\centering
%\includegraphics[width=1.95in]{graphics/robot.jpg}
%\subcaption{RC car}\label{fig:rccar}
%\end{minipage}
\begin{minipage}[b]{0.15\textwidth}\centering
\includegraphics[width=1in]{graphics/screen1.png}
\subcaption{Rectangle}\label{fig:rect}\end{minipage}
\begin{minipage}[b]{0.15\textwidth}\centering
\includegraphics[width=1in]{graphics/screen2.png}
\subcaption{Tight turns}\label{fig:turns}\end{minipage}
\begin{minipage}[b]{0.15\textwidth}\centering
\includegraphics[width=0.8in]{graphics/screen3.png}
\subcaption{Large clover}\label{fig:clover}\end{minipage}
\caption{Implementation and environments built in AirSim}
\label{fig:patrol-plan}
\end{figure*}

\begin{table*}
\caption{Average speed, Monitor failure rates, plant violation rates, for AirSim and human driver in Rectangle, Turns, and Clover for Patrol missions}
\label{tab:results}
\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l}
\bottomrule        & \multicolumn{5}{|c|}{Avg.\ Speed (m/s)} & \multicolumn{5}{c|}{Ctrl Fail.} & \multicolumn{5}{c|}{Plant Fail.} \\
World   & BB   & \textbf{PD1}   & PD2   & PD3     & Human     & BB     & \textbf{PD1}      & PD2    & PD3    & Human  & BB    & \textbf{PD1} & PD2   & PD3 & Human \\\toprule
Rect    & 9.2  & \textbf{6.24}   & 8.47 &  13.5   & 12.6      & 1.4\%  & \textbf{0\%}      & 0\%    & 0.17\% & 3.0\%  & 9.8\% & \textbf{1.8\%} & 1.2\%  & 4.0\%   & 10.5\%  \\
Turns   & 8.3  & \textbf{4.54}   & 5.93 &  10.4   & 10.8      & 3.0\%  & \textbf{0\%}      & 3.3\%  & 0.75\% & 3.5\%  & 11\%  & \textbf{0.2\%} & 0.4\%  & 3.1\%   & 5.63\%  \\
Clover  & 13.8 & \textbf{13.9}   & 18.2 &  29.8   & 29.3      & 6.6\%  & \textbf{0.3\%}    & 0\%    & 0.32\% & 0.66\% & 28\%  & \textbf{1.7\%} & 44\%   & 38.1\%  & 24.4\%
\end{tabular}
\end{table*}
\rref{tab:results} gives the results of several controllers (PD1 is the slowest PD controller, PD3 the fastest) as well as a human pilot (the author) on all environments.
Overall, the slow PD controller had the best monitor failure rates of any controller.
Qualitatively, the results tell us that while perfect failure 0\% rates are not always attainable, low failure rates are achievable in practice, where low control failure rates mean that the fallback controller will rarely need to engage and low plant failure rates mean that because physical assumptions are rarely violated, the formal guarantees provided by \VeriPhy are applicable almost all the time.

Aside from the concrete numbers provided by the simulations, perhaps the most important conclusion is that such a simulation could be developed and monitored in the first place.
Throughout the development of this section, many iterations of the models, proofs, and simulations were developed: in order to make the proofs feasible, the models would be simplified, then when monitor failures were detected, the models would be relaxed to improve the failure rate.
Low failure rates not only speak to the success of the implementation, but they say even more about the success of the model: any time the model were too restrictive to account for the values given by the AirSim sensors and controllers, the monitor would have failed.
The fact that they fail rarely attests that the model can account for the control decisions and physical dynamics that arise in practice.
Because it was not until this chapter that a completely formal link between \dL and CPS execution had been developed, this itself was long a point of curiosity.

\subsection{Related Work}
Related work in formal methods and robotics has applied synthesis and verification to safe control of robotic systems.
The present work is unique among these in its use of a verified-safe sandbox controller to enforce compliance between the implementation and formal model for a realistic robot simulation with an expressive correctness guarantee.

\subsubsection{Synthesis for Planning and Control}
A significant focus of related work is the synthesis of motion plans from high-level specifications.
These works can be fruitfully combined with ours, providing a high-level plan to be followed, but none of them address correctness of low-level controllers that implement the plan, and all trust the correctness of their kinematic models:
\begin{itemize}
\item The tools LTLMoP~\cite{DBLP:conf/iros/FinucaneJK10} and TuLiP~\cite{DBLP:conf/IEEEcca/FilippidisDLOM16} can synthesize robot controls that satisfy a high-level temporal logic specification.
However, they depend on accuracy of their kinematic models, which assume  discrete-space and discrete-time, and the guarantees are not end-to-end.
\item Bisimulation~\cite{871304} is used to synthesize plans based on hybrid dynamics~\cite{DBLP:conf/cdc/BhatiaKV10,DBLP:journals/automatica/FainekosGKP09}, but assumes model compliance and cannot ensure low-level controller correctness.
\item Controllers have been synthesized:
\begin{inparaenum}[\it i)]
\item from temporal logic specifications for linear systems~\cite{DBLP:journals/tac/KloetzerB08},
\item for adaptive cruise control~\cite{DBLP:journals/tcst/NilssonHBCAGOPT16}, tested in simulation and on hardware, and
\item from safety proofs~\cite{DBLP:conf/emsoft/TalyT10} for switched systems using templates,
\end{inparaenum}
but those works assume model compliance and cannot ensure low-level controller correctness.
\end{itemize}

\subsubsection{Offline Verification for Planning and Control}
In contrast to online synthesis of a correct plan and/or controller, offline verification shows correctness before the fact, which is often more tractable and eases incorporation of custom controllers.
Much of the verification in robotics focuses on hybrid systems models;
common approaches are reachability analysis~\cite{DBLP:conf/cav/FrehseGDCRLRGDM11} and theorem proving~\cite{DBLP:conf/cade/FultonMQVP15}.
\begin{itemize}
\item 1D straight-line motion was addressed both in \dL (for direct velocity control)~\cite{DBLP:conf/pldi/BohrerTMMP18} and with reachability analysis~\cite{chen2015benchmark},
 but the 2D motion demanded by real robots introduces novel modeling, verification, and implementation challenges: our invariants and controllers must account for curved motion, acceleration, and uncertain sensing and actuation.
\item
 Unbounded-time 2D obstacle avoidance and 1D liveness have also been proved in \dL~\cite{DBLP:conf/rss/MitschGP13,DBLP:journals/ijrr/MitschGVP17}.
A paper proof of liveness using \dL rules assumed perfect sensing and constant speed \cite{DBLP:journals/corr/abs-1709-02561}.
 Their controllers, like ours, are closely related to the classic Dynamic-Window~\cite{DBLP:journals/ram/FoxBT97} control algorithm.
 In contrast to the prior \dL effort, we offer stronger results, including 2D liveness, waypoint-following, and end-to-end correctness.
\item
  A planner for ground vehicles was verified~\cite{DBLP:conf/atva/RizaldiISA18} in Isabelle/HOL.
  Unlike our work, it does not address low-level control and implementation correctness.%, but could be used to provide an input plan.
\end{itemize}
%  We provide a simpler model which enabled LEEV of our implementations with \VeriPhy, whereas the model of~\cite{DBLP:journals/ijrr/MitschGVP17} is too restrictive (i.e., the synthesized monitor would always fail in practice).
%\item Reachability is most-often used in design, but can also be used for model-predictive control~\cite{Bemporad2000PerformanceDR}.
%Hybrid systems theorem-proving in \dL~\cite{DBLP:books/sp/Platzer18} with \KeYmaeraX~\cite{DBLP:conf/cade/FultonMQVP15} is typically less automated, with unique advantages in system complexity (polynomial ODEs), property complexity (unbounded-time safety and liveness), and rigor (small\cite{DBLP:conf/cade/FultonMQVP15}, verified~\cite{DBLP:conf/cpp/BohrerRVVP17} trusted base).  Control strategies for ground vehicles are well-studied.
%Reachability for liveness has been investigated~\cite{BEMPORAD2001382}.

\subsubsection{Online Verification}
In contrast to offline verification, online (or runtime) verification uses runtime checks to enforce safe operation.
\begin{itemize}
\item The basis of online verification is the SIMPLEX~\cite{Krogh1998TheSA} method, which uses a trusted monitor to decide between an untrusted controller and trusted fallback.
\item The \VeriPhy~\cite{DBLP:conf/pldi/BohrerTMMP18} toolchain for \dL, which we use, extends SIMPLEX by ensuring the monitor is correct-by-construction, formally proving the safety of the resulting system, and automatically maintaining safety down to machine code implementation.
High-Assurance SPIRAL~\cite{DBLP:journals/csm/FranchettiLMMGPPKMFJPV17} is another implementation toolchain for \dL, but does not provide formal end-to-end guarantees, and neither has been used to develop an end-to-end system capable of free-range 2D driving until now.
\item Runtime reachability analysis has been used for car control~\cite{DBLP:journals/trob/AlthoffD14}, but relies on correctness of the model and of the analysis implementation.
Since reachability tools rely on large, complicated codebases and models are challenging to get right, these assumptions present a correctness gap which stands in the way of an end-to-end argument.
\end{itemize}

\subsubsection{Simulation}
Simulation is an essential part of evaluating models and designs for any robotic system.
Multiple simulation platforms are available, of which AirSim~\cite{shah2018airsim} is a recent platform for UAVs and autonomous cars.
Other simulators would likely have worked as well, but we chose AirSim because it is configured with high-fidelity physical and visual models out of the box, reducing the chances of introducing modeling errors.

In short, while verification of robotics is well-studied, few works have addressed end-to-end guarantees with the highest level of rigor, and in this paper we develop the first realistic, end-to-end system with an expressive correctness guarantee of safety and liveness for 2D waypoint following.

\chapter{Proposed Work: \CdGL: Constructive Differential Game Logic}
\label{ch:cdgl}
In previous sections, the author has argued for end-to-end verification of CPS, as it has been persued thus far in \VeriPhy.
Here, it is argued that \VeriPhy as conceived for classical hybrid systems does not and fundamentally cannot tell the entire story of end-to-end verification for realistic CPS, and that a more compelling story is told by \emph{constructive} hybrid \emph{games}.
We begin this argument by discussing the limitations inherent to \VeriPhy for classical systems:
\begin{enumerate}
\item \label{item:adversarial} Real CPS are adversarial in the sense that even when the environment is not actively plotting our destruction, it is not under our control.
 Because we wish for our CPS to function correctly under all circumstances, we must assume the worst of the environment, which is assume adversarial behavior.
\item \label{item:complexified} A subtler point arises from the adversarial assumption, which we will show by example:
While CPS which are ``inherently adversial'' \emph{can actually} be modeled and verified as hybrid systems, the point is that such models are doubly unsatisfactory: On the one hand, they are more complicated, sometimes significantly so, than the game model would be, because the controller model must explictly write our the player's strategy for overcoming the adversarial environment, which in the case of games is left to the proof.
On the other hand, this added complexity does not provide a gain in generality but rather \emph{costs} generality, because the systems model gives only one particular strategy where the game might have had several available strategies for winning, each with its own tradeoffs.
These weaknesses fly directly in the face of the thesis that for end-to-end verification, picking the right question is half the battle, because the use of hybrid systems makes our questions simultaneously more complicated and less fruitful than necessary. They can hardly be called the right questions.
\item \label{item:realistic} While the distinction between systems and games is justified by our need to account for real adversarial dynamics and to do so elegantly, 
the choice of computational games over classical games is made for foundational reasons: CPS's contain actual computers which must actually compute, and thus their formal models must capture an appropriate notion of computation.
\item \label{item:computational} Because the computational content of a classical systems proof is limited, the potential of classical systems as a foundation for synthesis is limited.
\item \label{item:control-synthesis} In particular, because \VeriPhy only provides synthesis for control \emph{monitors} and not also synthesis for \emph{controllers}, it is not a comprehensive synthesis solution for CPS.
\end{enumerate}
We elaborate on points \ref{item:adversarial}, \ref{item:complexified}, and \ref{item:realistic} here, while we elaborate on points \ref{item:computational} and \ref{item:control-synthesis} in \rref{ch:proofplex}.
We compare the 1D car hybrid game model \rref{ex:driving-game} with the system model \rref{ex:driving-system}.

\newcommand{\gameex}{\alpha_{\times}\xspace}
\newcommand{\sysex}{\alpha_{\text{Sys}}\xspace}
What does adversarial behavior in games look like and how does it compare to cooperative behavior in systems?
For systems, we often prove safety theorems:
\[P \limply \dbox{\sysex}{Q}\]
which say that if $P$ holds initially, then \emph{for all} executions of $\sysex$, formula $Q$ holds at the end.
What makes this c\"ooperative is that the universal quantification applies to both the control and the plant, which is to say both are controlled by an agent whose decisions are unknown to the person performing the proof, and because they are unknown must be treated universally.
What is striking is that both the control and plant are pessimistic; in some real sense the proof of controller safety is trying its hardest to fail, and a controller is only safe if the \emph{model} is so restrictive  that it gives the safety \emph{proof} no choice but to conclude safety.
This can be contrasted with the dual property of co-safety:
\[P \limply \ddiamond{\sysex}{Q}\]
which says that if $P$ holds initially then there is always \emph{some} execution of $\sysex$ that ensures $Q$ as a postcondition.
Here the control and plant are again c\"ooperating, but now both are under control of the prover.
Whereas a safety proof, in its pessimism, results in overly complicated models, a cosafety theorem is typically simply wrong.
To control the plant is simply to control its duration.
It's entirely reasonable to suggest that the environment pessimistically controls the environment, i.e.,  that the plant might for whatever duration is ``most dangerous'' and then hand control back to the controller.
On the other hand, it is utterly unreasonable to suggest that the CPS can optimistically control the environment: we cannot perfectly control how long physics will evolve before we regain control, and we certainly cannot control phenomena such as sensing and actuation noise.
This is often addressed in practice by proving not co-safety, but liveness as in \rref{thm:liveness}:
\[P \limply \dbox{\sysex}{\ddiamond{\sysex}{Q}}\]
which is to say that no longer how the long the system has been running, if control is handed over to us at that point, we can control it to achieve the postcondition $Q$.
while this notion comes closer to an intuitive notion of liveness, there are two glaring artifacts: in the box $\dbox{\sysex}{}$ the pessimist has control of the controller, while in the diamond $\ddiamond{\sysex}{},$ the optimist has control of the plant.
The overarching message of this tangent is simple: regardless whether we wish to show safety or liveness, to characterize this properties precisely we should always give control of the controller to the optimist and control of the plant to the pessimist, which is exactly what games achieve!

Out of this discussion falls the following point, that models as they are written in \dL are simultaneously more complicated and less general than the simpler characterization in \dGL, and as a consequence the theorems we can state about them are less satisfactory.
We return again to \rref{ex:driving-game} and \rref{ex:driving-system}.
One can reasonably expect (say as an exercise in a senior-level theorem-proving course) to state and prove theoremss like safety of \rref{ex:driving-system}, and indeed it is not terribly difficult to convince ourselves that this system is actually safe.
Howeveer, it is significantly more difficult to convince ourselves of the following properties:
\begin{itemize}
\item Is it total, i.e., does the controller always have a branch it can take?
\item Is it exhaustive, allowing every safe control decision in every state?
\item Is it as simple as possible, or are some cases redundant?
\item Is the controller computable?
\end{itemize}
these questions are not trivial.
For example, questions of totality and exhaustiveness often lead novices to write entirely inappropriate models for which a so-called safety theorem is provable but which do not correspond to any reasonable notion of safety for an actual CPS and for which the synthesized \VeriPhy monitor would surprise them with a huge number of failures!
The simplicity question is noteworthy because verification is time-consuming: any time we promote simpler models and simpler proofs, we promote time-saving for the user.
Computability is essential to the synthesis question, and thus is a pressing motivation for \CdGL over \dGL.
Even the simple first-order arithmetic conditions given in this model are intriguing for the simple reason that we must ponder what it means to compute real arithmetic.
In this light, the advantages of games are startling: totality questions vanish because the controller is under Angelic control, and safety is simply not provable of a non-total controller.
Exhaustivity and simplicity vanish because the strategy is no longer part of the model but part of the proof: A non-simple proof is allowed, but its complexity cannot infect the model, which is infinitely more important because it must be taken as a given whereas a proof can be checked for compliance with the proof calculus.
Exhaustivity is now a question only for the proof as well, and is arguably not even \emph{desired}: now that a strategy characterizes the exact decision that would be taken under every circumstance, there is no longer a strict need to prove the safety of operations that will not actually be taken.
Computability is addressed not by \dGL but by \CdGL.

These questions of computability are technically and foundationally fascinating.
The solution taken by \VeriPhy, while workable, is simplistic: all arithmetic is interpreted as fixed-point interval arithmetic.
This is limiting because it is conservative: any time that a computation exceeds the provided word size or even demands a higher precision, control monitors will fail spuriously because the computed intervals violated monitor conditions even though the result of an exact computation would not.
There are two obvious approaches for eliminating the conservativity limitation:
\begin{enumerate}
\item Perform all arithmetic with an exact representation, among which plausible choices include rational numbers, real-algebraic numbers, and constructive reals.
\item Retain the use of fixed-precision arithmetic, but provide a mechanism to prove that a particular precision is sufficient, for example by accounting for physical limits on the numerical range of inputs.
\end{enumerate}
Each approach has advantages and disadvantages: The arbitrary-precision approach likely lends itself to simpler formal semantics and simpler proofs, because the semantics can assume once and for all that numbers have arbitrary precision and proofs need not concern themeselves with precision.
The implications for synthesis for hard real-time systems are problematic, however, because implementations of arbitrary-precision arithmetic are neither constant-time nor constant-space, and thus violate a number of industry standards for usage in hard real-time systems.
Fixed-precision arithmetic, on the other hand, may complicate semantics in that the semantics may need to be parameterized by precision, and may complicate proof in that proofs might have to be aware of the precision.
However, once those problems are addressed, the implications are far better for synthesis for usage in real-time systems, as the resulting code is constant-time and constant-space, in compliance with industrial standards.

Several other interesting computational, foundational questions arise.
Because our programming language contains differential equations, what does it mean to compute a system of differential equations?
How does this differ for a differential equation controlled by us vs. an adversary?
Both cases are important: monitoring the behavior of an adversarial differential equation is essential for synthesizing plant monitors, while computing the behavior of an angelic differential equation enables \emph{model-predictive} controllers, which make control decisions directly by projecting forward how the plant will respond to the control decision, a fundamental trope in control systems.

Lastly, the interaction between constructive logic and first-order arithmetic is itself fascinating: In constructive logic, we deny the law of the excluded middle, but for first-order arithmetic, the law of the excluded middle in incontrovertibly true, because first-order arithmetic is \emph{decidable}!
How then do we manage the intermingling of arithmetic questions which are entitled by their decidability to classicality with dynamic-logical questions which are rightly constrained by the needs of synthesis to constructivity?
There are again several approaches: one can introduce a modality for classical truth, embedding classical questions inside a constructive logic.
One can also sneak around the question by relegating first-order arithmetic to the \emph{term language}: it is well-known that the set of states where any first-order arithmetic formula holds true can be written as a \emph{semi-algebraic set}.
One can simply extend the term language with semi-algebraic sets of reals, and with a membership-testing formula, which is clearly decidable by the decidability of semi-algebraic sets.
This approach seems particularly appealing because it both provides a clear exposition of the relationship between a computational hybrid game and the code synthesized from it, and at the same (and to be clear, this is entirely icing on the cake) provides a beautiful connection with the complete work on definite descriptions (\rref{sec:definite-description}): definite descriptions and indefinite descriptions are in fact both computable so long as their descriptions are semi-algebraic, and can be reduced to singleton and arbitrary semi-algebraic sets, respectively.
Thus there is great reason to believe that by taking this approach, we could also provide the same generality in the term language that was found in \rref{sec:definite-description}.
As a general note, we present these alternative design decisions in the proposal not only to argue in favor of the preferred approaches, but to show that alternative approaches are available should the preferred approach run into an insurmountable stumbling block during work on the thesis.

%\textrm_{\alpha{Sys}}

\section{Related Work}
\newcommand{\semiset}[2]{\{#1~|~#2\}}
\section{Syntax}
In order to account for decidable first-order arithmetic, I propose adding semi-algebraic sets explicitly to the syntax of the term language.
A particularly radical choice is define terms as \emph{only} the semi-algebraic sets, i.e.:
\[\theta \bebecomes \semiset{x}{F}\]
where $F$ is a first-order arithmetic formula which may itself also refer to further semialgebraic sets, and where $x$ is a real-valued variable optionally appearing in $F$.
The argument in favor of this choice is that real-valued terms can be derived from semialgebraic sets because they are simply singleton sets.
Prior experience shows that incorporating multiple datatypes is a potential headache, and this would eliminate the need for multiple types.
The arguments against it include the desire to syntactically distinguish reals vs sets, the fact that the derived constructs may be cryptic to readers, and that it's unclear whether differential terms are implementable with semialgebraic sets alone.
For that reason it is perhaps more advisable to give a full term language:
\[\theta \bebecomes \semiset{x}{F} \alternative c \alternative x \alternative \theta + \eta \alternative \theta \cdot \theta \alternative \der{\theta}\]
We could also choose to interpret a semialgebraic set as a nondeterministic term, in which case an indefinite description chosen from $F$ is identified with $F$ itself.
Definite description is derivable from indefinite description as usual.
Basic set operations are derivable.

The core formulas are probably going to be:
\[\phi \bebecomes \theta \sim \eta \alternative \theta \subseteq \theta \alternative \phi \land \psi \alternative \phi \lor \psi \alternative \phi \limply \psi \alternative \bot \alternative \ddiamond{\alpha}{\phi} \alternative \dbox{\alpha}{\phi} \alternative \forall{x}{\phi} \alternative \exists{x}{\phi}\]
Where $\sim \in \{<, \leq\}$.
While other comparison operators on terms should be derivable, it is not clear whether $<$ and $\leq$ are interderivable because the derivation would be by negation.
This is not the case for set inclusions because set inclusions can use classical reasoning including complements.
In contrast with classical \dL, more operators are built in because less operators are interderivable in constructive logics.
As usual in constructive logics, $\neg \phi$ is defined as $\phi \limply \bot$ and equivalences are definable as usual even in classical logic: $\phi \lequiv \psi \equiv \phi \limply \psi \land \psi \limply \phi$.

The hybrid game constructs of \CdGL are those of \dGL, though of course the formulas and terms contained therein are those of \CdGL again.

\section{Example System}

\section{Semantics}
A major question is what numeric types should be used in the semantics of \CdGL.
For \CdGL to truly be a ``computational'' logic, one could fairly argue that the semantics ought to capture computations with realistic types.
For reasons of managing complexity, however, I propose starting with actual real numbers initially, then developing alternative semantics which can be related to the real semantics, much as was done in \cite{DBLP:conf/pldi/BohrerTMMP18}.
Some of the alternative approaches have glaring holes: computable reals often don't terminate which seems horribly inelegant, while assigning a rational or even real-algebraic number as the denotation of each number is problematic because the rationals and real-algebraics are not closed under evolution of differential equations.
The solution to computability may be found first (and even surprisingly) in the solution to giving an intuitionic meaning to the propositional connectives: intuitionistic Kripke models work with sets of states (where the real state might be anywhere in that region, but we know only that we are somewhere in the region).

So I propose first developing a semantics over real-valued regions (sets of states), which can capture the intuitionistic meaning of the connectives while leaving arithmetic issues open.
Two viable approaches for then handling computational issues would be either to replace arbitrary regions with semi-algebraic regions, or to replace regions over the reals with regions over the rationals.
Semi-algebraic regions may be beautiful in that they give us a dynamic representation for our uncertainty, which we can compute with directly.
Rationals are beautiful in their own way, which is that in the implementation we can compute with rationals and justify it by the rational-valued semantics, but then somehow tie that back to the real-valued semantics presumably with the fact that reals can be constructed as Dedekind cuts, which are just sets of rationals anyway.
That is, any semantics that computes with arbitrary sets of rationals ought to be powerful enough to do anything a real number could do anyway.
Certainly the typical issue of nonstrict comparisons with reals should be expected to appear at some point in the development.
One potential way out instead of using nonterminating computations we have a semantics ``up to $\epsilon$'' where rationals are guaranteed to be within an error bound of the actual real semantics.
In the ``problematic'' case, then simply we say that the relevant formulas are neither true nor false (which is already totally natural in a constructive logic) and we can characterize the gross inexact inequalities as the case where the rational semantics does not converge to the real semantics.
But we still have a theorem that saying when it converges to anything at all, it converges to the real semantics.

\subsection{Real Intuitionistic Systems Semantics}
We begin with an intuitionistic semantics for just systems (not games) based are real numbers.
This obviously does not deal with issues relating to how arithmetic gets implemented.
It is unclear whether the use of real numbers will introduce incompleteness issues in the proof calculus.

The semantics are based on intuitionstic Kripke frames.
In typical abstract treatments, each intuitionistic world is in some sense epistemic: it represents which facts are currently known to be true.
A preorder $\leq$ is used to represent which worlds are ``at least as knowledgeful'' as some other state.
We take the same general approach that intuitionistic states represent the available knowledge, except the presentation is concrete.
%\newcommand{\allregion}{\mathcal{R}}
\newcommand{\allcon}{\allregion}
\newcommand{\somesemi}[2]{\epsilon #1~|~#2}
Where $\allvars$ is the countable set of identifiers, the set of program states $\allstate$ is $\mathbb{R}^\allvars$, then an intuitionistic state $X : \allcon \equiv 2^\allstate$ is simply a (usually uncountably infinite) set of states, and to ``know'' an intuitionistic state $X$ is to know that we must be in some $\omega \in X,$ but not \emph{which} $\omega$.
Special case $X = \allcon$ means we know nothing, $X = \{\omega\}$ means that we know we are in exactly the state $\omega,$ and $X = \emptyset$ means we know with certainty that we have derived a contradiction, as we know we are in a state that belongs to the empty set, of which there are none.
The preorder $\leq$ on intuitionisic worlds is also given concretely: $X \leq Y \equiv X \supseteq Y$: that is, if $Y$ is a subset of $X$ then it contains more information than $Y$ because it rules out all of the states that $Y$ does, plus some more.

%There are several design options for the term semantics: do we include semi-algebraic set computations in the term language, and if so do we make a type distinction or identify sets with nondetermistic terms?
%One can argue that semialgebraic sets or an equally powerful construct are \emph{necessary} in the term language (not just a nice-to-have because they make it more powerful) because without them we do not have the existence property (EP), which we will prove after introducing the semantics.
%We also choose not to make a typing distinction in terms since it the technical complexity of introducing a type system is not well motivated by a system of exactly two types.
%An alternative to explore would be to make the typing distinction, then have a family of operators for drawing elements from sets.

Therefore we will choose $\tint{\theta}{\om} : \mathbb{R}$: the value of a term in some state is its unique value.
The semantics are defined by:
\begin{align*}
  \tint{q}{\om} &= q\\
  \tint{x}{\om} &= \om(x)\\
  \tint{\theta + \eta}{\om} &= \tint{\theta}{\om} + \tint{\eta}{\om}\\
  \tint{\theta \cdot \eta}{\om} &= \tint{\theta}{\om} \cdot \tint{\eta}{\om}\\
  \tint{\der{\theta}}{\om} &= \sum_{x\in\allvars}\frac{\partial\tint{\theta}{\om}}{\partial x}\om(\D{x})
%  \tint{\somesemi{x}{F}}{\om} &= \{r \in \mathbb{R}~|~ \fint{F}{\subst[\om]{x}{r}}\}
\end{align*}
I%t is arguable whether semialgebraic set terms should be allowed to be non-unique; if they are required to be unique then the semantics need not be deterministic in the first place.
%The issue is primarily one of what it means to be intuitionistic:  if we are computing for example with rational numbers or lists of real-algebraic numbers, then it would make no sense to consider a semi-algebraic set computable.
%Yet the whole point of using semialgebraic sets in \CdGL is that they are themselves a well-understood data structure for computer algebra, so can certainly be considered computable.
%That being said, we will be forced to reconsider this decision once we present the program semantics, because nondeterminism in terms has a deep impact on nondeterminism and constructivity in programs.

We give the semantics for formulas.
Because this is intuitionistic truth, the semantics are a partial function and words like ``and'' and ``or'' are interpreted partially, for example conjunction is neither true nor false if either conjunct is neither true nor false.
Here $F$ ranges over semialgebraic sets.
\begin{align*}
  \fint{\phi \land \psi}{X}     &\lequiv \fint{\phi}{X}\text{ and }\fint{\psi}{X}\\
  \fint{\phi \lor \psi}{X}      &\lequiv \fint{\phi}{X}\text{ or }\fint{\psi}{X}\\
% TODO: Prove a monotonicity lemma to simplify this
  \fint{\phi \limply \psi}{X}   &\lequiv \text{ for all }Y \subseteq X, \fint{\phi}{Y}\text{ implies }\fint{\psi}{Y}\\
  \fint{\bot}{X}                &\lequiv X = \emptyset\\
  \fint{\theta > \eta}{X}       &\lequiv \text{for all }\nu \in X, \exists \epsilon > 0, \text{for all } \mu \in (B_\epsilon(\nu) \cap X), \tint{\theta}{\mu} > \tint{\eta}{\mu}\\
  \fint{\exists{x}{\phi}}{X}    &\lequiv \fint{\phi}{\{\subst[\om]{x}{f(\om)}~|~\om \in X\}} \text{ for some $f:\allstate \to \mathbb{R}$}\\
  \fint{\forall{x}{\phi}}{X}    &\lequiv \fint{\phi}{\{\subst[\om]{x}{r}~|~\om \in X,  r \in \mathbb{R}\}}\\
  \fint{\dbox{\alpha}{\phi}}{X} &\lequiv \text{for all }Y \subseteq X, Z \text{ s.t. }(Y,Z) \in \pint{\alpha}, \fint{\phi}{Z}\\
  \fint{\ddiamond{\alpha}{\phi}}{X} &\lequiv \text{exists }Y, (X,Y)\in\pint{\alpha}\text{ and }\fint{\phi}{Y}\\
  \fint{\theta \geq \eta}{X}    &\lequiv \fint{\eta > \theta \limply \bot}{X}%\text{for all }r\in \tint{\theta}{\om}, s\in\tint{\nu}, \om,\nu \in X, r \geq s\\
\end{align*}

We give the semantics for programs:
\begin{align*}
  (X,X) \in \pint{\ptest{\phi}}\text{ iff }&\fint{\phi}{X} \\
  (X,Y) \in \pint{\humod{x}{\theta}} \text{ iff }& Y = \{\subst[\om]{x}{r}~|~\om \in X, r \in \tint{\theta}{X}\}\\
  \pint{\alpha \cup \beta} =& \pint{\alpha} \cup \pint{\beta}\\
  (X,Y) \in \pint{\alpha;~\beta} \text{ iff }& \text{exists }Z, (X,Z) \in \pint{\alpha}, (Z,Y) \in \pint{\beta}\\
  (X,Y) \in \pint{\pevolvein{\D{x}=\theta}{\ivr}}\text{ iff }& Y = \{\nu~|~\exists \om \in X, t \in \mathbb{R}_{\geq0}, \varphi_\om:[0,t]\to\allstate,~\varphi_\om(0)=\om\text{ on }\{x,\D{x}\}^C\\
   &\frac{\partial \varphi_\om(t)(x)}{\partial t}= \varphi_\om(t)(\D{x})\text{ and } \{\varphi_\om(t)(x)\}=\tint{\theta}{\varphi(\varphi(s))}\\
   &\text{ and }\fint{\ivr}{\{\varphi_\mu(t)~|~\mu\in X}\}\\
  \pint{\prepeat{\alpha}} =& \bigcup_{n\in\mathbb{N}} \pint{\alpha}^n
\end{align*}

How do we evaluate whether these semantics ought to be considered intuitionistic?
One excellent test is whether the \emph{existence property} (EP) holds.
The EP formalizes that anytime an existence property is proven, an actual witness to the existential property can be computed as well.
In a dynamic logic, we can express two such EP's:
\begin{lemma}[Term EP]
If $\Gamma \limply \exists{x}{\phi(x)}$ is true, then there is a term $\theta$ such that $\Gamma \limply \phi(\theta)$ is true.
Moreover, $\theta$ can be computed from given a proof of $\Gamma \limply \exists{x}{\phi(x)}$.
\end{lemma}
\begin{proof}[Sketch.]
  We will need to introduce a proof calculus before we can speak to the proof aspect, but such a $\theta$ trivially exists by the semantics because $\fint{\exists{x}{\phi}}{X}$ specifically assumes
 the existance of a semialgebraic witness $F$ which is then also a term.
\end{proof}

\begin{lemma}[Program EP]
If $\Gamma \limply \ddiamond{\alpha}{\phi}$ is true, then there exists an execution path $\rho$ (let's say that an execution path is straight-line deterministic program that refines the original) of $\alpha$ such that  
  $\Gamma \limply \ddiamond{\rho}{\phi}$ or equivalently $\Gamma \limply \dbox{\rho}{\phi}$ is true.
  Moreover it is possible to compute such a path from a proof of $\Gamma \limply \ddiamond{\alpha}{\phi}$.
\end{lemma}
\begin{proof}
  Because paths are not already part of the language, one would need to define what paths are (and convince us that they're computational enough).
  But first lets consider some of the requirements for such a property to hold.
  By back-of-the-envelope cardinal arithmetic, any program with uncountably many paths is not terribly promising, because we must get lucky that some path satisfying $\phi$ could be represented by one of the countably many path programs.
 What if, for example, $\alpha$ is the program $\humod{x}{1};~\humod{y}{0};~\humod{t}{0};~\pevolvein{\D{x}=-y,\D{y}=x,\D{t}=1}{y \geq 0};\ptest{x=-1}$?
 The only end state for this program is $\{x=-1,y=0,t=\pi\}$.
 How would we hope to construct a straight line program $\rho$ which constructs this state? 
 We certainly can't do so without differential equations, and even with a differential equation we must somehow specify its duration of $\pi$.
 Admittedly, we can't prove that this would be impossible until we specify what exactly we want paths to be, but something is incredibly suspicious here: $\pi$ is transcendental and our straight-line programs can only construct real-algebraic numbers because they are equipped only with semialgebraic sets.
 We can't even write down the duration of the ODE (allowing paths to run an ODE so long as they write down the duration is a plausible approach to intuitionistic ODEs).
 If we restrict the durations say to be only the real-algebraic numbers, we can now write down paths using semialgebraic sets, but also the program above becomes empty because we cannot express the duration as a real algebraic number.
 In contrast if we worked with computable reals we'd be fine here.
 This all really just goes to say that the field we operate over will be super important. Hmm...
 Anyway, where I was originally trying to go with this (and got sidetracked by other issues with ODEs) is that if you only have countably-many paths through the program, then in some sense EP should be semi-decidable even without a proof of $\ddiamond{\alpha}{\phi}$ because you can just iterate through the paths of $\alpha$.
 And then it's guaranteed to terminate if you've proven the diamond because this is all just countable choice anyway which is fine constructively if you ask nicely enough.
 But it's quite likely all of this is terrifically irrelevant and will become clear once I work out a proof calculus.
\end{proof}

{\Huge TODOs} (in order to figure out best semantics to make EP true):
\begin{itemize}
\item Read References from Fischer-Servi (intuitionistic-base.pdf)
\item Contemplate disjunctive property
\item Literature review: denotatonal semantics of intuitionistic logics
\item Learn constructive analysis
%\item Meditate on bloating and whether one should care whether numbers are closed under ODE evaluation
\item Review Scott continuity. Reflect on relationship between Scott continuity, interval arithmetic and constructive reals.
  It would seem that the epistemic nature of intuitionism should allow us to find a uniform foundation for an interval semantics or for a constructive reals semantics: i.e. if your intervals are too big in interval arithmetic, you just don't know stuff. In constructive reals you keep asking for smaller intervals until you do know. If you can't know without asking for infinitely many intervals, then you just don't know.
Reflect on the interactions with interval bloating around ODEs that might happen say in rationals case, as well. Perhaps this is all fine.
%\item Heyting algebra
%\item Review EP from  Degen + Werner (done, is about k's for *'s and requires proofs).
%\item Review Bas Spitters Picard Lindeloef in Coq + RosCoq while we're at it to convince myself computable reals are the main computable thing closed under ODE evaluation (done)
\end{itemize}
\section{Proof Calculus}

\section{Soundness Theorem}
\section{Relaxing (Constructive) Games to (Constructive) Systems}
\section{Winning Strategy Satisfies The Relaxation}
\section{Reducing (Constructive) Games to (Classical) Games}
%Corollary: by composition, constructive games reduce under whatever silly assumptions to classical gamesx


\chapter{Proposed Work: \ProofPlex, Proof-Directed Game Synthesis}
\label{ch:proofplex}
In this chapter we propose a tool for the synthesis of controllers and monitors from (proven-safe and proven-live) computational hybrid games, which we name \ProofPlex.
We motivate \ProofPlex by first considering the different artifacts one might wish to synthesize:
\begin{enumerate}
\item A \emph{controller monitor}, which detects whether an untrusted external controller is compliant with the assumptions of a controller model
\item A \emph{plant monitor}, which detects whether the environment is compliant with the physical assumptions of the plant model.
\item A \emph{controller}, which actually picks a guaranteed-safe-and-live control decision to carry out
\item A \emph{plant simulator}, which picks an adversarial behavior for the plant and evaluates the effect of the plant.
\end{enumerate}

All of these are desirable under different circumstances:
Synthesizing a controller is appealing because unlike an untrusted controller, it is guaranteed to make a safe decision so the feedback need not be invoked (i.e, it is proven to satisfy the controller monitor).
Yet the downside of a synthesized controller is that it sacrifices flexibility: if our synthesized monitor guarantees correctness but was not optimized for operational efficiency, fuel efficiency etc., we might one day decide we pefer a controller which meets those criteria instead.
In this case the flexibility given by the monitioring approach is preferred.
On the plant side, plant monitors are perfect for deployment in production systems or during field tests: they allow us to assess whether assumptions on the environment are met in practice.
But in early stages of development, especially if there is not a purpose built simulator for our system already, we would much prefer to synthesize code which can actually (and faithfully) simulate the plant for us, which is helpful in designing the corresponding controller.
The first two are monitors which merely detect noncompliance with a model, whereas the latter two actively control the system.
Notably, the first two are supported by the existing \ModelPlex tool while the latter two are not.
Moreover, we obviously wish that we could synthesize these artifacts for arbitrarily complicated systems, whereas existing implementations of \ModelPlex have limitations, for example that the controller must contain no differential equations and loops.
Until recent advances, it was even more restrictive, with plants restricted to only nilpotent differential equations.

It is by looking at the advances that led to recent gains that we can then, however, discover the changes that achieve our lofty goal of synthesizing all four artifacts and doing so for arbitrary systems (and games).
A key insight behind a recent advance in the \ModelPlex tool (which generalized plants to polynomial ODEs so long as the plant is proven safe using only chains of inductive invariants) is that even in synthesizing a monitor, \emph{the contents of a proof are essential to synthesis}.
Specifically, that work extracted differential invariants from a safety proof in order to synthesize monitors
In retrospect, this key insight is no longer surpising: synthesis should be at least as difficult as verification, and because verification of hybrid systems/games is undecidable, a general-case synthesis procedure should not exist.
Yet even though verification is undecidable, proof \emph{checking} is utterly decidable, as the proof contains all the difficult decisions made in determining the truth of a liveness statement.
Therefore, the ultimate question behind this chapter is, ``can the proof content be reused for synthesis''?, to which the thesis statement answers ``yes''.
A proof in \CdGL is nothing but a winning strategy for a hybrid game, which contains both all the control decisions made by the Angelic player and the assumptions monitored of the Demonic player.
In short, the following could be called the punchline of the thesis:
\[\text{A box proof witnesses monitorability and a diamond witnesses controllability, thus a \CdGL proof witnesses synthesizability}\]
Whenever the players alternate turns in a game, the proof alternates modalities.
Whenever we prove a $\ddiamond{\alpha}{P}$ property, we do so by witnessing the decisions (computations) Angel performs to achieve $P$.
In proving a $\dbox{\alpha}{P}$ property, we enumerate rules which Angel must uphold until control passes to Demon, and in doing so we enumerate the contents of a monitor.

The subthesis of this chapter is that this content can and should be used to automate the synthesis of all four artifacts, and that it can do so not just in special cases but in general cases, including:
\begin{itemize}
\item Model-predictive controllers using differential equations
\item Differential equations with not only differential invariants but also differential ghosts, which together suffice to prove any polynomial invariant of a polynomial ODE
\item Controllers containing loops
%TODO: Model a nuclear power plant so I can have a nonatomic plant of an atomic plant :D
\item Nonatomic plants which for example employ different ODEs in different cases, or may employ sequences of ODEs to model different stages of a system
\end{itemize}

The implementation of \ProofPlex contains an implementation of the relaxation theorems of the previous section, which first characterize which classical proofs also work as constructive proofs, then compute a hybrid system relaxation of the game.
Once a relaxation for the controller model (and controller proof) in a hybrid game has been computed, verification can be made end-to-end by hooking it in to the existing \VeriPhy pipeline.
What remains is to supply the untrusted controller.
Because \ProofPlex, unlike \ModelPlex, also synthesizes monitors by exploiting angelic proofs, we automatically provide an ``untrusted'' controller which in fact is trustworthy, because it simply an implementation of the (operational) angelic dynamics of the control proof in \CdGL. By the work in the previous chapter, these operational semantics refine the standard denotational semantics of both the hybrid game and of its systems relaxation, meaning they satisfy the monitor.

\subsection{Related Work}
\chapter{Conclusion} 
\section{Timeline}
The timeline given below is from the time this document is completed (say, April 2019).
Due to geographic issues, I expect to propose in early Fall (Sep 2019).
If all goes according to plan, I intend to defend in late Spring 2020.
%I also intend to go on the academic job market Fall 2019.
%Realistically some of the time below will be spent on job searching.
%I have tried to make my timeline permissive enough to allow time for that.

Total time: 13 months
\begin{itemize}
\item 1 month: Extend \dL formalization in Isabelle to support \dLi
\item 1 month: Refine \KeYmaeraX to \Isabelle interface, ensure sufficient support for case study
\item 4 months: Develop \CdGL theory (Submit to POPL in July 2019)
\item 2 months: Implement \CdGL proof checking
\item 2 months: Develop \ProofPlex implementation (Submit in January 2020) 
\item 1 month: Generalize ground robotics case study to \CdGL
\item 2 months: Write thesis document
\end{itemize}

\appendix

\backmatter

%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
\bibliographystyle{plainnat}
\bibliography{proposal,platzer,verified-pipeline,hilbert-epsilons,ground-robotics,verified-dL} %your bib file


\end{document}
